{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import subprocess\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tabpfn import TabPFNRegressor\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_cuda(model):\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"CUDA memory cleared and model deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_best_corr_with_target(X, X_val, X_control, y, threshold=0.6, df_columns=None, number_of_features=40):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    correlation_matrix = X.corrwith(y).abs()\n",
    "    to_keep = correlation_matrix.sort_values(ascending=False).head(number_of_features).index\n",
    "    X = X[to_keep]\n",
    "    X_val = X_val[to_keep]\n",
    "    X_control = X_control[to_keep]\n",
    "    return X.to_numpy().copy(), X_val.to_numpy().copy(), X_control.to_numpy().copy()\n",
    "\n",
    "def feature_extraction_with_Pearson(X, X_val, X_control, y, threshold=0.6, df_columns=None):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    correlation_matrix = X.corr().abs()\n",
    "    upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    X = X.drop(columns=to_drop)\n",
    "    X_val = X_val.drop(columns=to_drop)\n",
    "    X_control = X_control.drop(columns=to_drop)\n",
    "    return X.to_numpy().copy(), X_val.to_numpy().copy(), X_control.to_numpy().copy()\n",
    "\n",
    "def feature_extration_with_PCA(X, X_val, X_control, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(X), pca.transform(X_val), pca.transform(X_control)\n",
    "\n",
    "def feature_extration_with_BE(X, X_val, X_control, y, significance_level=0.05, df_columns=None):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    X = sm.add_constant(X)\n",
    "    while True:\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        p_values = model.pvalues\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > significance_level:\n",
    "            feature_to_remove = p_values.idxmax()\n",
    "            print(f\"Removing {feature_to_remove} with p-value {max_p_value:.4f}\")\n",
    "            X = X.drop(columns=[feature_to_remove])\n",
    "            X_val = X_val.drop(columns=[feature_to_remove])\n",
    "            X_control = X_control.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "        print(\"Final Feature length: \", len(X.columns))\n",
    "    X_ret = X.drop(columns=['const']).to_numpy().copy()\n",
    "    return X_ret, X_val.to_numpy().copy(), X_control.to_numpy().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_coefficient(y_true, y_pred):\n",
    "    if len(y_true) <= 1 or len(y_pred) <= 1:\n",
    "        raise ValueError(\"Pearson correlation requires at least two points in each array.\")\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true and y_pred must have the same length.\")\n",
    "\n",
    "    # Convert input to pandas series (if not already)\n",
    "    y_true = pd.Series(y_true).astype(int)\n",
    "    y_pred = pd.Series(y_pred).astype(int)\n",
    "\n",
    "    # Check for NaNs or infinite values\n",
    "    if y_true.isna().any() or y_pred.isna().any():\n",
    "        raise ValueError(\"Input contains NaN values.\")\n",
    "    if not np.isfinite(y_true).all() or not np.isfinite(y_pred).all():\n",
    "        raise ValueError(\"Input contains infinite values.\")\n",
    "\n",
    "    # Compute and return the correlation\n",
    "    result = y_true.corr(y_pred)\n",
    "    if np.isnan(result):\n",
    "        return 0.0\n",
    "    return result\n",
    "\n",
    "def evaluate_regression_performance(y_true, y_pred, title=\"\", round_predictions=True):\n",
    "    if round_predictions:\n",
    "        y_pred = np.round(y_pred).astype(int)\n",
    "        y_true = np.round(np.array(y_true)).astype(int)\n",
    "    else:\n",
    "        y_pred = np.array(y_pred).astype(float)\n",
    "        y_true = np.array(y_true).astype(float)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    pearson = pearson_correlation_coefficient(y_true, y_pred)\n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'pearson': pearson\n",
    "    }\n",
    "    print(f\"\\n {title} Regressor Performance:\")\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, Pearson: {pearson:.4f}\")\n",
    "    return results\n",
    "\n",
    "def aggregate_cv_metrics_and_print(all_results, model_name, tag=\"Validation\"):\n",
    "    aggregated = {'mse': [], 'mae': [], 'r2': [], 'pearson': []}\n",
    "    for result in all_results:\n",
    "        aggregated['mse'].append(result['mse'])\n",
    "        aggregated['mae'].append(result['mae'])\n",
    "        aggregated['r2'].append(result['r2'])\n",
    "        aggregated['pearson'].append(result['pearson'])\n",
    "    summary = {\n",
    "        'mean_mse': np.mean(aggregated['mse']),\n",
    "        'std_mse': np.std(aggregated['mse']),\n",
    "        'mean_mae': np.mean(aggregated['mae']),\n",
    "        'std_mae': np.std(aggregated['mae']),\n",
    "        'mean_r2': np.mean(aggregated['r2']),\n",
    "        'std_r2': np.std(aggregated['r2']),\n",
    "        'mean_pearson': np.mean(aggregated['pearson']),\n",
    "    }\n",
    "    print(f\"\\n {model_name} Regressor Performance {tag}:\")\n",
    "    for k, v in summary.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col= \"age_at_assessment\"\n",
    "straticify_col = \"NONE\"\n",
    "straticify_col_test = \"NONE\"\n",
    "filter_for_sex = 0\n",
    "\n",
    "os.makedirs(\"/opt/notebooks/TABPFN/02_UKB/00_data/age_label\", exist_ok=True)\n",
    "os.makedirs(\"../00_data/confounded/\", exist_ok=True)\n",
    "mri_table = \"aparc.thickness_aparc.volume_aseg.volume.csv\"\n",
    "\"\"\" # Load the age data\n",
    "command = \"dx download file-GyGfBQ8J34gPK8XXxbjYGbg4 --output /opt/notebooks/TABPFN/02_UKB/00_data/age_label/all_ages_all_ids_healthy.csv --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "#load mri data\n",
    "command = f\"dx download file-GyGf9vjJ34g2g9QbJQ7P1qZG --output '/opt/notebooks/TABPFN/02_UKB/00_data/deconfounded_but_age/{mri_table}' --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True) \"\"\"\n",
    "\n",
    "# Load the age data middle\n",
    "#command = \"dx download file-GyJp51jJ34g246Y7bZ6j7yK4 --output /opt/notebooks/TABPFN/02_UKB/00_data/age_label/all_ages_all_ids_healthy.csv --overwrite\"\n",
    "#subprocess.run(command, shell=True, check=True)\n",
    "#load mri data cleand and renamed but age\n",
    "#command = f\"dx download file-GyJp6B0J34g8xpf6Q6jz12xJ --output '/opt/notebooks/TABPFN/02_UKB/00_data/deconfounded_but_age/{mri_table}' --overwrite\"\n",
    "#subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "# Load the mri data\n",
    "command = f\"dx download file-GyQXZf0J34g7bJK06XB2vZQx --output '../00_data/confounded/{mri_table}' --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "# Load the age data that is just healthy subjects from UKB\n",
    "command = \"dx download file-GyQY91QJ34gFY8Fv28Pqzp0v --output /opt/notebooks/TABPFN/02_UKB/00_data/age_label/healthy_subjects_train.csv --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "df = pd.read_csv(f\"../00_data/confounded/{mri_table}\")\n",
    "df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "label_df = pd.read_csv(\"/opt/notebooks/TABPFN/02_UKB/00_data/age_label/healthy_subjects_train.csv\")\n",
    "label_df[\"ID\"] = label_df[\"ID\"].astype(str)\n",
    "#just get the ids where the sex is 0\n",
    "print(label_df[\"sex\"].value_counts())\n",
    "label_df = label_df[label_df[\"sex\"] == filter_for_sex]\n",
    "print(label_df[\"sex\"].value_counts())\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "if straticify_col != \"NONE\":\n",
    "    label_df = label_df[['ID', label_col, straticify_col]]\n",
    "    merged_df = pd.merge(df, label_df, on='ID', how='inner')\n",
    "    merged_df.dropna(inplace=True)\n",
    "    label_counts = merged_df[straticify_col].value_counts()\n",
    "\n",
    "    # Include all rows for groups with fewer samples than the target threshold\n",
    "    threshold = 1000  # You can adjust this threshold as needed\n",
    "    small_groups = label_counts[label_counts <= threshold].index\n",
    "    small_groups_df = merged_df[merged_df[straticify_col].isin(small_groups)]\n",
    "\n",
    "    # Calculate how many more samples are needed to reach 10,000\n",
    "    remaining_needed = 10000 - len(small_groups_df)\n",
    "\n",
    "    # Sample proportionally from the larger groups\n",
    "    large_groups = label_counts[label_counts > threshold].index\n",
    "    large_groups_df = merged_df[merged_df[straticify_col].isin(large_groups)]\n",
    "\n",
    "    # Stratified sampling from the remaining data\n",
    "    proportional_sampled_df, _ = train_test_split(\n",
    "        large_groups_df, \n",
    "        train_size=remaining_needed, \n",
    "        stratify=large_groups_df[straticify_col], \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Combine the small groups and the proportional sample\n",
    "    final_sampled_df = pd.concat([small_groups_df, proportional_sampled_df])\n",
    "\n",
    "    # Verify the result\n",
    "    print(final_sampled_df[straticify_col].value_counts())\n",
    "    print(f\"Total samples: {len(final_sampled_df)}\")\n",
    "else:\n",
    "    label_df = label_df[['ID', label_col]]\n",
    "\n",
    "    merged_df = pd.merge(df, label_df, on='ID', how='inner')\n",
    "    merged_df.dropna(inplace=True)\n",
    "    final_sampled_df = merged_df.sample(n=10000, random_state=42)\n",
    "if label_col != straticify_col and straticify_col != \"NONE\":\n",
    "    final_sampled_df.drop(columns=[straticify_col], inplace=True)\n",
    "final_sampled_df[label_col].hist()\n",
    "print(len(final_sampled_df))\n",
    "df_sampled = final_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test data complete\n",
    "command = \"dx download file-GyQXkxQJ34gB2qXXQyPGZv7g --output /opt/notebooks/TABPFN/02_UKB/00_data/age_label/Matched_validation_sick_healthy.csv --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "df_sick_healthy = pd.read_csv(\"/opt/notebooks/TABPFN/02_UKB/00_data/age_label/Matched_validation_sick_healthy.csv\")\n",
    "#rename the column sick to label_sick\n",
    "df_sick_healthy.rename(columns={\"sick\": \"label_sick\"}, inplace=True)\n",
    "df_sick_healthy_sex = df_sick_healthy[df_sick_healthy[\"sex\"] == filter_for_sex]\n",
    "label_df_control = df_sick_healthy_sex[df_sick_healthy[\"label_sick\"] == 0]\n",
    "label_test_sick = df_sick_healthy_sex[df_sick_healthy[\"label_sick\"] == 1]\n",
    "label_df_control[\"ID\"] = label_df_control[\"ID\"].astype(int).astype(str)\n",
    "label_test_sick[\"ID\"] = label_test_sick[\"ID\"].astype(int).astype(str)\n",
    "print(label_df_control[\"sex\"].value_counts())\n",
    "print(label_test_sick[\"sex\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/opt/notebooks/TABPFN/02_UKB/00_data/validation_data/00_National_Cohort/\", exist_ok=True)\n",
    "#load middle age control data\n",
    "command = \"dx download file-GyK09JQJ34g95zyvV9vFxQFv --output /opt/notebooks/TABPFN/02_UKB/00_data/validation_data/00_National_Cohort/all_ages_all_ids_subset_middle_age.csv --overwrite\"\n",
    "#subprocess.run(command, shell=True)\n",
    "#load mri data\n",
    "command = \"dx download file-GyK08xjJ34g95zyvV9vFxQFf --output /opt/notebooks/TABPFN/02_UKB/00_data/validation_data/00_National_Cohort/aparc.thickness_aseg.volume_aparc.volume_deconfounded_but_age.csv --overwrite\"\n",
    "#subprocess.run(command, shell=True)\n",
    "\n",
    "df_control = pd.read_csv(f\"../00_data/confounded/{mri_table}\")\n",
    "df_control[\"ID\"] = df_control[\"ID\"].astype(str)\n",
    "df_control[\"ID\"] = df_control[\"ID\"].str.replace(\"sub-\", \"\")\n",
    "if straticify_col_test != \"NONE\":\n",
    "    label_df_control = label_df_control[['ID', straticify_col_test, label_col]]\n",
    "    merged_df_control = pd.merge(df_control, label_df_control, on='ID', how='inner')\n",
    "    merged_df_control.dropna(inplace=True)\n",
    "    #sample 400 so that from each group 25 samples if possible\n",
    "    target_samples_per_group = 25\n",
    "    grouped_df = merged_df_control.groupby(straticify_col_test)\n",
    "\n",
    "    # Sample 25 from each group if possible, otherwise sample all available\n",
    "    sampled_control = grouped_df.apply(lambda x: x.sample(n=min(target_samples_per_group, len(x)), random_state=42))\n",
    "\n",
    "    # Reset the index after sampling\n",
    "    sampled_control.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Check if we reached the desired total number of 400 samples\n",
    "    if len(sampled_control) < 400:\n",
    "        print(f\"Only {len(sampled_control)} samples available after balanced sampling.\")\n",
    "    else:\n",
    "        print(f\"Sampled {len(sampled_control)} rows with balanced distribution across groups.\")\n",
    "else:\n",
    "    label_df_control = label_df_control[['ID', label_col]]\n",
    "    merged_df_control = pd.merge(df_control, label_df_control, on='ID', how='inner')\n",
    "    merged_df_control.dropna(inplace=True)\n",
    "    sampled_control = merged_df_control\n",
    "    #sample randomly 400 samples\n",
    "    sampled_control = sampled_control.sample(n=1000, random_state=42)\n",
    "X_control_source = sampled_control.drop([\"ID\", label_col], axis=1)\n",
    "y_control_source = sampled_control[label_col]\n",
    "control_ids = sampled_control[\"ID\"]\n",
    "y_control_source.hist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_control = df_sampled.drop([label_col, \"ID\"], axis=1).columns\n",
    "X_control = X_control_source[column_control]\n",
    "y_control_max = y_control_source.max()\n",
    "y_control_min = y_control_source.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_control_max, y_control_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_sampled))\n",
    "len(X_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_control.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "best_pearson =0.0\n",
    "best_model_path = None\n",
    "best_model_type = None\n",
    "# You can adjust these percentages as needed\n",
    "#percentage_of_the_data = [1.0, 0.8, 0.6, 0.5, 0.2, 0.05]\n",
    "percentage_of_the_data = [0.05, 0.2, 0.5, 0.6, 0.8, 1.0]\n",
    "percentage_of_the_data = [0.05]\n",
    "FE_strategy = [\"Nothing\", \"BE\", \"PCA\", \"Correlation_in_Feature\", \"Correlation_with_target\"]\n",
    "FE_strategy = [\"Nothing\"]\n",
    "\n",
    "# Dictionary to store aggregated CV metrics for each percentage and deconfounding strategy\n",
    "percentage_dict = {}\n",
    "\n",
    "# Lists to record individual predictions (for test and control sets)\n",
    "test_predictions_records = []    # will include real outcomes and predictions (test set from CV)\n",
    "control_predictions_records = [] # will include real outcomes and predictions (control set)\n",
    "\n",
    "# We will also record random-baseline metrics (using random predictions drawn from uniform [0,1])\n",
    "random_results = []            # for test set performance\n",
    "random_results_eval = []       # for control set evaluation (if desired)\n",
    "\n",
    "# For each deconfounding strategy and percentage, we will also collect model metrics\n",
    "# We will collect separate lists for test-set (\"cv_results\") and control-set (\"cv_results_eval\")\n",
    "for percentage in percentage_of_the_data:\n",
    "    percentage_dict[percentage] = {}\n",
    "    \n",
    "    # Subsample the training data as needed\n",
    "    if percentage == 1:\n",
    "        df_sampled_subset = df_sampled.copy()\n",
    "    else:\n",
    "        df_sampled_subset, _ = train_test_split(\n",
    "            df_sampled,\n",
    "            train_size=percentage,\n",
    "            random_state=42\n",
    "        )\n",
    "    print(f\"\\n #### TRAINING WITH {percentage} OF THE DATA ####\")\n",
    "    # Separate IDs, outcomes, and features\n",
    "    ids = df_sampled_subset[\"ID\"]\n",
    "    y = df_sampled_subset[label_col]\n",
    "    X = df_sampled_subset.drop([\"ID\", label_col], axis=1)\n",
    "    \n",
    "    print(f\"Training data shape: {X.shape}, number of samples: {len(y)}\")\n",
    "    \n",
    "    for FE_ext in FE_strategy:\n",
    "        print(f\"\\n=== FE-Ext: {FE_ext} ===\")\n",
    "        \n",
    "        # Prepare lists for storing CV metrics (for test set and for control set)\n",
    "        tabpfn_results = []\n",
    "        random_results_model = []  # for test-set random baseline\n",
    "        \n",
    "        tabpfn_results_eval = []\n",
    "        random_results_eval_model = []  # for control-set random baseline (if desired)\n",
    "        \n",
    "        # Create a KFold object (using KFold for regression)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_counter = 0\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            fold_counter += 1\n",
    "            print(f\"\\nFold {fold_counter}\")\n",
    "            X_train, X_test = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
    "            y_train, y_test = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
    "            test_ids = ids.iloc[val_index].copy()\n",
    "            \n",
    "            # Use the entire control set (IDs are stored in control_ids)\n",
    "            X_control = X_control_source.copy()\n",
    "            y_control = y_control_source.copy()\n",
    "            try:\n",
    "                X_control = X_control[X_train.columns]\n",
    "            except Exception as e:\n",
    "                print(\"Columns mismatch between training and control:\", e)\n",
    "            \n",
    "            # Scale the data\n",
    "            df_columns = X_train.columns\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            X_control_scaled = scaler.transform(X_control)\n",
    "            \n",
    "            # Apply deconfounding / feature extraction strategy\n",
    "            if FE_ext == \"BE\":\n",
    "                X_train_proc, X_test_proc, X_control_proc = feature_extration_with_BE(\n",
    "                    X_train_scaled, X_test_scaled, X_control_scaled, y_train, df_columns=df_columns)\n",
    "            elif FE_ext == \"PCA\":\n",
    "                X_train_proc, X_test_proc, X_control_proc = feature_extration_with_PCA(\n",
    "                    X_train_scaled, X_test_scaled, X_control_scaled, n_components=50)\n",
    "            elif FE_ext == \"Correlation_in_Feature\":\n",
    "                X_train_proc, X_test_proc, X_control_proc = feature_extraction_with_Pearson(\n",
    "                    X_train_scaled, X_test_scaled, X_control_scaled, y_train, threshold=0.6, df_columns=df_columns)\n",
    "            elif FE_ext == \"Correlation_with_target\":\n",
    "                X_train_proc, X_test_proc, X_control_proc = feature_extraction_best_corr_with_target(\n",
    "                    X_train_scaled, X_test_scaled, X_control_scaled, y_train, threshold=0.6, df_columns=df_columns)\n",
    "            elif FE_ext == \"Nothing\":\n",
    "                X_train_proc, X_test_proc, X_control_proc = X_train_scaled, X_test_scaled, X_control_scaled\n",
    "            \n",
    "            #############################\n",
    "            # RANDOM BASELINE\n",
    "            #############################\n",
    "            n_samples_test = len(y_test)\n",
    "            # Random predcition between min and max\n",
    "            random_pred = np.random.uniform(y_test.min(), y_test.max(), n_samples_test)\n",
    "            random_pred = np.round(random_pred)\n",
    "            random_metrics = evaluate_regression_performance(y_test, random_pred, title=\"Random - Test\")\n",
    "            random_results_model.append(random_metrics)\n",
    "            \n",
    "            # For control set, random predictions as well:\n",
    "            n_samples_control = len(y_control)\n",
    "            random_control_pred = np.random.uniform(y_test.min(), y_test.max(), n_samples_control)\n",
    "            random_metrics_eval = evaluate_regression_performance(y_control, random_control_pred, title=\"Random - Control\")\n",
    "            random_results_eval_model.append(random_metrics_eval)\n",
    "            \n",
    "            # Record random predictions in the prediction logs (for test set)\n",
    "            for idx, true_val, pred_val in zip(test_ids, y_test, random_pred):\n",
    "                test_predictions_records.append({\n",
    "                    'ID': idx,\n",
    "                    'fold': fold_counter,\n",
    "                    'model': 'Random',\n",
    "                    'real_outcome': true_val,\n",
    "                    'predicted_outcome': pred_val,\n",
    "                    'percentage': percentage,\n",
    "                    'deconfounding': FE_ext\n",
    "                })\n",
    "            # And for control set\n",
    "            for idx, true_val, pred_val in zip(control_ids, y_control, random_control_pred):\n",
    "                control_predictions_records.append({\n",
    "                    'ID': idx,\n",
    "                    'fold': fold_counter,\n",
    "                    'model': 'Random',\n",
    "                    'real_outcome': true_val,\n",
    "                    'predicted_outcome': pred_val,\n",
    "                    'percentage': percentage,\n",
    "                    'deconfounding': FE_ext\n",
    "                })\n",
    "            \n",
    "            #############################\n",
    "            # TABPFN Regressor\n",
    "            #############################\n",
    "            tabpfn_model = TabPFNRegressor()\n",
    "            tabpfn_model.fit(X_train_proc, y_train)\n",
    "            tabpfn_pred = tabpfn_model.predict(X_test_proc)\n",
    "            tabpfn_metrics = evaluate_regression_performance(y_test, tabpfn_pred, title=\"tabpfn - Test\")\n",
    "            tabpfn_results.append(tabpfn_metrics)\n",
    "            \n",
    "            # Record tabpfn predictions (test set)\n",
    "            for idx, true_val, pred_val in zip(test_ids, y_test, tabpfn_pred):\n",
    "                test_predictions_records.append({\n",
    "                    'ID': idx,\n",
    "                    'fold': fold_counter,\n",
    "                    'model': 'tabpfn',\n",
    "                    'real_outcome': true_val,\n",
    "                    'predicted_outcome': pred_val,\n",
    "                    'percentage': percentage,\n",
    "                    'deconfounding': FE_ext\n",
    "                })\n",
    "            # Evaluate on control set\n",
    "            tabpfn_control_pred = tabpfn_model.predict(X_control_proc).flatten()\n",
    "            tabpfn_control_metrics = evaluate_regression_performance(y_control, tabpfn_control_pred, title=\"tabpfn - Control\")\n",
    "            tabpfn_results_eval.append(tabpfn_control_metrics)\n",
    "            # Record control predictions for tabpfn\n",
    "            for idx, true_val, pred_val in zip(control_ids, y_control, tabpfn_control_pred):\n",
    "                control_predictions_records.append({\n",
    "                    'ID': idx,\n",
    "                    'fold': fold_counter,\n",
    "                    'model': 'tabpfn',\n",
    "                    'real_outcome': true_val,\n",
    "                    'predicted_outcome': pred_val,\n",
    "                    'percentage': percentage,\n",
    "                    'deconfounding': FE_ext\n",
    "                })\n",
    "            clean_up_cuda(tabpfn_model)\n",
    "            \n",
    "        #Get the best model with the best metric in resutl for control set\n",
    "        for model, metric in zip([\"tabpfn\"], [tabpfn_control_metrics[\"pearson\"]]):\n",
    "            if metric > best_pearson:\n",
    "                best_pearson = metric\n",
    "                if model == \"tabpfn\":\n",
    "                    best_model_path = f'/opt/notebooks/{percentage}_{FE_ext}_{model}_{metric}'\n",
    "                    best_model_type = \"tabpfn\"\n",
    "                    tabpfn_model.save_model(best_model_path)\n",
    "                print(f\"Best model saved with pearson {best_pearson} to {best_model_path}\")\n",
    "        # Aggregate and print performance for each model (test set)\n",
    "        random_summary = aggregate_cv_metrics_and_print(random_results_model, \"Random\")\n",
    "        tabpfn_summary = aggregate_cv_metrics_and_print(tabpfn_results, \"tabpfn\")\n",
    "\n",
    "        # Aggregate for control set evaluations\n",
    "        random_eval_summary = aggregate_cv_metrics_and_print(random_results_eval_model, \"Random\", tag=\"Control\")\n",
    "        tabpfn_eval_summary = aggregate_cv_metrics_and_print(tabpfn_results_eval, \"tabpfn\", tag=\"Control\")\n",
    "        \n",
    "        # Save results in the dictionary\n",
    "        percentage_dict[percentage][FE_ext] = {\n",
    "            \"Random\": {\n",
    "                \"results\": random_summary,\n",
    "                \"results_eval\": random_eval_summary,\n",
    "                \"cv_results\": random_results_model,\n",
    "                \"cv_results_eval\": random_results_eval_model\n",
    "            },\n",
    "            \"tabpfn\": {\n",
    "                \"results\": tabpfn_summary,\n",
    "                \"results_eval\": tabpfn_eval_summary,\n",
    "                \"cv_results\": tabpfn_results,\n",
    "                \"cv_results_eval\": tabpfn_results_eval\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "    # Set these flags as desired\n",
    "    Feature_extraction_applied = False\n",
    "    Pretraining_applied = False\n",
    "    # You can set these flags based on the deconfounding strategy if needed.\n",
    "\n",
    "    all_rows = []\n",
    "    log_file = \"/opt/notebooks/results_regression.csv\"\n",
    "    # Iterate over percentages and their associated models\n",
    "    for percentage, models in percentage_dict.items():\n",
    "        for feat_ext, feature_summary_dict in models.items():\n",
    "            for model_name, summary_dict in feature_summary_dict.items():\n",
    "                # Each summary_dict contains aggregated metrics as well as CV lists.\n",
    "                # Iterate over the number of folds (using the cv_results list)\n",
    "                for i, (cv_result, cv_result_eval) in enumerate(zip(summary_dict[\"cv_results\"], summary_dict[\"cv_results_eval\"])):\n",
    "                    # Prepare training (test set) row\n",
    "                    row_train = {\n",
    "                        \"label_col\": label_col,\n",
    "                        \"mri_table\": mri_table,\n",
    "                        \"test_set_size\": f\"{(1 - percentage):.2%} (approx. of data left for test)\",\n",
    "                        \"Feature_extraction_applied\": Feature_extraction_applied,\n",
    "                        \"Pretraining_applied\": Pretraining_applied,\n",
    "                        \"model_type\": model_name,\n",
    "                        \"mse\": cv_result.get(\"mse\", None),\n",
    "                        \"mae\": cv_result.get(\"mae\", None),\n",
    "                        \"r2\": cv_result.get(\"r2\", None),\n",
    "                        \"pearson\": cv_result.get(\"pearson\", None),\n",
    "                        \"number_of_cross_validations\": n_splits,\n",
    "                        \"cross_validation_count\": i,\n",
    "                        \"search_term\": f\"{percentage}_{feat_ext}_{model_name}_train\",\n",
    "                        \"percentage_of_data\": percentage,\n",
    "                        \"eval_or_train\": \"train\"\n",
    "                    }\n",
    "                    # Prepare evaluation (control set) row\n",
    "                    row_eval = {\n",
    "                        \"label_col\": label_col,\n",
    "                        \"mri_table\": mri_table,\n",
    "                        \"test_set_size\": f\"{(1 - percentage):.2%} (approx. of data left for test)\",\n",
    "                        \"Feature_extraction_applied\": Feature_extraction_applied,\n",
    "                        \"Pretraining_applied\": Pretraining_applied,\n",
    "                        \"model_type\": model_name,\n",
    "                        \"mse\": cv_result_eval.get(\"mse\", None),\n",
    "                        \"mae\": cv_result_eval.get(\"mae\", None),\n",
    "                        \"r2\": cv_result_eval.get(\"r2\", None),\n",
    "                        \"pearson\": cv_result_eval.get(\"pearson\", None),\n",
    "                        \"number_of_cross_validations\": n_splits,\n",
    "                        \"cross_validation_count\": i,\n",
    "                        \"search_term\": f\"{percentage}_{feat_ext}_{model_name}_eval\",\n",
    "                        \"percentage_of_data\": percentage,\n",
    "                        \"eval_or_train\": \"eval\"\n",
    "                    }\n",
    "                    all_rows.append(row_train)\n",
    "                    all_rows.append(row_eval)\n",
    "\n",
    "    # Convert to DataFrame and save CSV\n",
    "    df_results = pd.DataFrame(all_rows)\n",
    "    df_results.to_csv(log_file, index=False)\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_results = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_results.to_csv(log_file, index=False)\n",
    "    logs_path = \"project-GqzxkVQJ34g6ygFJ4ZbvqBYF:/Esra/00_CLIP/01_training_logs/\"\n",
    "    label = os.environ.get(\"DX_JOB_ID\") \n",
    "    logs_path_label = os.path.join(logs_path, label)\n",
    "    dx_mkdir_command = f\"dx mkdir '{logs_path_label}'\"\n",
    "    subprocess.run(dx_mkdir_command, shell=True)\n",
    "    time_tag = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    command_csv = f\"dx upload '{log_file}' --path '{logs_path_label}/{time_tag}_result_baseline.csv'\"\n",
    "    subprocess.run(command_csv, shell=True)\n",
    "\n",
    "    df_test_predictions = pd.DataFrame(test_predictions_records)\n",
    "    df_control_predictions = pd.DataFrame(control_predictions_records)\n",
    "\n",
    "    test_csv_path = \"/opt/notebooks/regression_test_predictions.csv\"\n",
    "    control_csv_path = \"/opt/notebooks/regression_control_predictions.csv\"\n",
    "\n",
    "    df_test_predictions.to_csv(test_csv_path)\n",
    "    df_control_predictions.to_csv(control_csv_path)\n",
    "\n",
    "    print(f\"Saved test predictions to {test_csv_path}\")\n",
    "    print(f\"Saved control predictions to {control_csv_path}\")\n",
    "    command_csv = f\"dx upload '{test_csv_path}' --path '{logs_path_label}/{time_tag}_test_predictions.csv'\"\n",
    "    subprocess.run(command_csv, shell=True)\n",
    "    command_csv = f\"dx upload '{control_csv_path}' --path '{logs_path_label}/{time_tag}_control_predictions.csv'\"\n",
    "    subprocess.run(command_csv, shell=True)\n",
    "#upload best model\n",
    "name = best_model_path.split(\"/\")[-1]\n",
    "command_csv = f\"dx upload '{best_model_path}' --path '{logs_path_label}/{time_tag}_{name}'\"\n",
    "subprocess.run(command_csv, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_instance():\n",
    "    job_id = os.environ.get(\"DX_JOB_ID\")\n",
    "    if job_id:\n",
    "        print(f\"Terminating job: {job_id}\")\n",
    "        # Terminate the job using dx terminate\n",
    "        subprocess.run([\"dx\", \"terminate\", job_id], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_participant(target_row, df_candidates, relax_order, used_ids,thresholds):\n",
    "    df_candidates = df_candidates[~df_candidates['ID'].isin(used_ids)].copy()\n",
    "    for relax_level in relax_order:\n",
    "        filtered = df_candidates.copy()\n",
    "        for criterion, match_exact in relax_level.items():\n",
    "            if criterion in ['assessment_centre']:\n",
    "                if match_exact:\n",
    "                    filtered = filtered[filtered[criterion] == target_row[criterion]]\n",
    "            elif criterion in ['sex']:\n",
    "                if match_exact:\n",
    "                    #find opposite sex\n",
    "                    filtered = filtered[filtered[criterion] != target_row[criterion]]\n",
    "            else:\n",
    "                if match_exact:\n",
    "                    diff = abs(filtered[criterion] - target_row[criterion])\n",
    "                    filtered = filtered[diff <= thresholds[criterion]]\n",
    "            if filtered.empty:\n",
    "                break\n",
    "        if not filtered.empty:\n",
    "            return filtered.iloc[0]\n",
    "    print(\"NO candidate found\")\n",
    "    return None\n",
    "relax_order = [\n",
    "    {'assessment_centre': True, 'deprivation_index': True, 'bmi': True, 'age_at_assessment': True},\n",
    "    {'assessment_centre': False, 'deprivation_index': True, 'bmi': True, 'age_at_assessment': True},\n",
    "    {'assessment_centre': False, 'deprivation_index': False, 'bmi': True, 'age_at_assessment': True},\n",
    "    {'assessment_centre': False, 'deprivation_index': False, 'bmi': False, 'age_at_assessment': True},\n",
    "    {'assessment_centre': False, 'deprivation_index': False, 'bmi': False, 'age_at_assessment': False},\n",
    "]\n",
    "thresholds = {\n",
    "    'age_at_assessment': 2,\n",
    "    'bmi': 3,\n",
    "    'deprivation_index': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"dx download file-GyQZgzQJ34g4xxYbB6BJJYPB --output /opt/notebooks/merged_multitarget_df_with_demographics_wm.csv --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "all_df = pd.read_csv(\"/opt/notebooks/merged_multitarget_df_with_demographics_wm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all that are not label_bad_memory 1 to 0 in col label_bad_memory\n",
    "all_df[\"label_Bad_WM_Memory\"] = all_df[\"label_Bad_WM_Memory\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "#nans too\n",
    "all_df[\"label_Bad_WM_Memory\"] = all_df[\"label_Bad_WM_Memory\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"label_Bad_WM_Memory\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"ID\"] = all_df[\"ID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all IDs from all_df that are in df_sampled\n",
    "all_df = all_df[~all_df[\"ID\"].isin(df_sampled[\"ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"label_Bad_WM_Memory\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just get the ones where label_sick is 0 for the negative groupe\n",
    "all_df_neg = all_df[all_df[\"label_sick\"] == 0]\n",
    "all_df_wm_pos = all_df[all_df[\"label_Bad_WM_Memory\"] == 1]\n",
    "full_df = pd.concat([all_df_neg, all_df_wm_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.value_counts(\"label_Bad_WM_Memory\")\n",
    "full_df = full_df[full_df[\"sex\"] == filter_for_sex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "def match_for_label(label, merged_df, relax_order, thresholds):\n",
    "    positive_group = merged_df[merged_df[label] == 1]\n",
    "    negative_group = merged_df[merged_df[label] == 0]\n",
    "\n",
    "    matches = []\n",
    "    used_ids = set() \n",
    "    for _, target_row in positive_group.iterrows():\n",
    "        matched = match_participant(target_row, negative_group, relax_order, used_ids, thresholds)\n",
    "        if matched is not None:\n",
    "            matches.append(matched)\n",
    "            used_ids.add(matched['ID'])\n",
    "    matched_df = pd.DataFrame(matches)\n",
    "    return label, matched_df, positive_group\n",
    "\n",
    "#label_cols = [col for col in df.columns if \"label\" in col]\n",
    "label_cols = [\"label_Bad_WM_Memory\"]\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(match_for_label)(label, full_df, relax_order, thresholds) for label in label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, matched_df, positive_group in results:\n",
    "    print(f\"Matched {len(matched_df)} participants for label {label}\")\n",
    "    print(f\"Positive group size: {len(positive_group)}\")\n",
    "    print(f\"Negative group size: {len(matched_df)}\")\n",
    "    print(f\"Matched group size: {len(matched_df)}\")\n",
    "    df = pd.concat([positive_group, matched_df])\n",
    "df[\"ID\"] = df[\"ID\"].astype(int).astype(str)\n",
    "print(df[\"sex\"].value_counts())\n",
    "print(df[\"label_Bad_WM_Memory\"].value_counts())\n",
    "#df = df[df[\"sex\"] == filter_for_sex]\n",
    "\n",
    "bad_wm = df[df[\"label_Bad_WM_Memory\"] == 1]\n",
    "good_wm = df[df[\"label_Bad_WM_Memory\"] == 0]\n",
    "\n",
    "bad_wm[\"age_at_assessment\"].hist()\n",
    "good_wm[\"age_at_assessment\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the age for each group with the best model\n",
    "mir_df = pd.read_csv(f\"../00_data/confounded/{mri_table}\")\n",
    "mir_df[\"ID\"] = mir_df[\"ID\"].str.replace(\"sub-\", \"\")\n",
    "merged_bad_wm = pd.merge(mir_df, bad_wm, on='ID', how='inner')\n",
    "merged_good_wm = pd.merge(mir_df, good_wm, on='ID', how='inner')\n",
    "y_bad_wm = merged_bad_wm[label_col]\n",
    "X_bad_wm = merged_bad_wm[mir_df.columns]\n",
    "X_bad_wm.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "y_good_wm = merged_good_wm[label_col]\n",
    "X_good_wm = merged_good_wm[mir_df.columns]\n",
    "X_good_wm.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "#predict the age for each group with the best model\n",
    "# Load the model\n",
    "if best_model_type == \"tabpfn\":\n",
    "    model = TabPFNRegressor()\n",
    "    model.load_model(best_model_path)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_bad_wm_scaled = scaler.fit_transform(X_bad_wm)\n",
    "X_good_wm_scaled = scaler.transform(X_good_wm)\n",
    "\n",
    "# Apply deconfounding / feature extraction strategy\n",
    "#skkip for now\n",
    "\n",
    "# Predict the age\n",
    "if best_model_type == \"tabpfn\":\n",
    "    y_pred_bad_wm = model.predict(X_bad_wm_scaled)\n",
    "    y_pred_good_wm = model.predict(X_good_wm_scaled)\n",
    "\n",
    "# save to csv\n",
    "bad_wm[\"predicted_age\"] = y_pred_bad_wm\n",
    "good_wm[\"predicted_age\"] = y_pred_good_wm\n",
    "\n",
    "bad_wm.to_csv(f\"/opt/notebooks/bad_wm_predicted.csv\", index=False)\n",
    "good_wm.to_csv(f\"/opt/notebooks/good_wm_predicted.csv\", index=False)\n",
    "\n",
    "#upload the files\n",
    "command = f\"dx upload '/opt/notebooks/bad_wm_predicted.csv' --path '{logs_path_label}/{time_tag}_bad_wm_predicted.csv'\"\n",
    "subprocess.run(command, shell=True)\n",
    "command = f\"dx upload '/opt/notebooks/good_wm_predicted.csv' --path '{logs_path_label}/{time_tag}_good_wm_predicted.csv'\"\n",
    "subprocess.run(command, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_wm[\"predicted_age\"].describe())\n",
    "bad_wm[\"age_at_assessment\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminate_instance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
