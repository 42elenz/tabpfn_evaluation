{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tabpfn import TabPFNClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import gc\n",
    "import torch\n",
    "from tensorflow.keras import backend as K\n",
    "import  statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        layers.Dense(1024, activation=\"relu\", input_shape=(input_shape,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu', input_shape=(input_shape[0], 1)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_cuda(model):\n",
    "    # Delete the Keras model\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    \n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Free CUDA memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    \n",
    "    print(\"CUDA memory cleared and model deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_extraction_best_corr_with_target(X,X_val, X_control, y, threshold=0.6, df_columns=None, number_of_features=40):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    correlation_matrix = X.corrwith(y).abs()\n",
    "    to_keep = correlation_matrix.sort_values(ascending=False).head(number_of_features).index\n",
    "    X = X[to_keep]\n",
    "    X_val = X_val[to_keep]\n",
    "    X_control = X_control[to_keep]\n",
    "    X_ret = X.to_numpy().copy()\n",
    "    X_val_ret = X_val.to_numpy().copy()\n",
    "    X_control_ret = X_control.to_numpy().copy()\n",
    "    return X_ret, X_val_ret, X_control_ret\n",
    "\n",
    "\n",
    "def feature_extraction_with_Pearson(X, X_val, X_control, y, threshold=0.6, df_columns=None):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    correlation_matrix = X.corr().abs()\n",
    "    upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    X = X.drop(columns=to_drop)\n",
    "    X_val = X_val.drop(columns=to_drop)\n",
    "    X_control = X_control.drop(columns=to_drop)\n",
    "    X_ret = X.to_numpy().copy()\n",
    "    X_val_ret = X_val.to_numpy().copy()\n",
    "    X_control_ret = X_control.to_numpy().copy()\n",
    "    return X_ret, X_val_ret, X_control_ret\n",
    "\n",
    "def feature_extration_with_PCA(X, X_val, X_control, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_control_pca = pca.transform(X_control)\n",
    "    return X_pca, X_val_pca, X_control_pca\n",
    "\n",
    "def feature_extration_with_BE(X, X_val, X_control, y, significance_level=0.05, df_columns=None):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    # Add constant for intercept\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    while True:\n",
    "        # Fit the OLS model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Get the p-values for each feature\n",
    "        p_values = model.pvalues\n",
    "        \n",
    "        # Find the feature with the highest p-value\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value > significance_level:\n",
    "            # Remove the feature with the highest p-value\n",
    "            feature_to_remove = p_values.idxmax()\n",
    "            print(f\"Removing {feature_to_remove} with p-value {max_p_value:.4f}\")\n",
    "            X = X.drop(columns=[feature_to_remove])\n",
    "            X_val = X_val.drop(columns=[feature_to_remove])\n",
    "            X_control = X_control.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "        print(\"Final Feature lengthe: \", len(X.columns))\n",
    "    # Return the final selected feature set (excluding the intercept)\n",
    "    X_ret = X.drop(columns=['const']).to_numpy().copy()\n",
    "    X_val_ret = X_val.to_numpy().copy()\n",
    "    X_control_ret = X_control.to_numpy().copy()\n",
    "    return X_ret, X_val_ret, X_control_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def print_model_performance(results):\n",
    "    \"\"\"\n",
    "    Print model performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Performance metrics from evaluate_model_performance()\n",
    "    \"\"\"\n",
    "    for metric, value in results.items():\n",
    "        if metric == 'classification_report':\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(value)\n",
    "        else:\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {value}\")\n",
    "def aggregate_cv_metrics_and_print(all_results, model_name, tag=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Aggregate cross-validation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    all_results (list): List of results dictionaries from each fold\n",
    "    \n",
    "    Returns:\n",
    "    dict: Aggregated metrics with means and standard deviations\n",
    "    \"\"\"\n",
    "    # Initialize aggregation dictionary\n",
    "    aggregated = {\n",
    "        'accuracy': [],\n",
    "        'balanced_accuracy': [],\n",
    "        'random_balanced_accuracy': [],\n",
    "        'roc_auc': []\n",
    "    }\n",
    "    \n",
    "    # Collect metrics from each fold\n",
    "    for result in all_results:\n",
    "        aggregated['accuracy'].append(result['accuracy'])\n",
    "        aggregated['balanced_accuracy'].append(result['balanced_accuracy'])\n",
    "        aggregated['random_balanced_accuracy'].append(result['random_balanced_accuracy'])\n",
    "        aggregated['roc_auc'].append(result['roc_auc'])\n",
    "    # Compute mean and standard deviation\n",
    "    summary = {\n",
    "        'mean_accuracy': np.mean(aggregated['accuracy']),\n",
    "        'std_accuracy': np.std(aggregated['accuracy']),\n",
    "        'mean_balanced_accuracy': np.mean(aggregated['balanced_accuracy']),\n",
    "        'std_balanced_accuracy': np.std(aggregated['balanced_accuracy']),\n",
    "        'mean_random_balanced_accuracy': np.mean(aggregated['random_balanced_accuracy']),\n",
    "        'std_random_balanced_accuracy': np.std(aggregated['random_balanced_accuracy']),\n",
    "        'mean_roc_auc': np.mean(aggregated['roc_auc']),\n",
    "        'std_roc_auc': np.std(aggregated['roc_auc'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n {model_name} Classifier Performance {tag}:\")\n",
    "    print_model_performance(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/opt/notebooks/TABPFN/02_UKB/00_data/age_label\", exist_ok=True)\n",
    "os.makedirs(\"/opt/notebooks/TABPFN/02_UKB/00_data/deconfounded_but_age\", exist_ok=True)\n",
    "mri_table = \"aseg.volume_aparc.volume_aparc.thickness.csv\"\n",
    "# Load the age data\n",
    "command = \"dx download file-GyGfBQ8J34gPK8XXxbjYGbg4 --output /opt/notebooks/TABPFN/02_UKB/00_data/age_label/all_ages_all_ids_healthy.csv --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "#load mri data\n",
    "command = f\"dx download file-GyGf9vjJ34g2g9QbJQ7P1qZG --output '/opt/notebooks/TABPFN/02_UKB/00_data/deconfounded_but_age/{mri_table}' --overwrite\"\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "df = pd.read_csv(f\"../00_data/deconfounded_but_age/{mri_table}\")\n",
    "label_df = pd.read_csv(\"../00_data/age_label/WM_digits_remembered.csv\")\n",
    "n_splits = 5\n",
    "label_col= \"Good_WM_Memory\"\n",
    "\n",
    "label_df = label_df[['ID', 'Good_WM_Memory']]\n",
    "merged_df = pd.merge(df, label_df, on='ID', how='inner')\n",
    "merged_df.dropna(inplace=True)\n",
    "df_sampled, _ = train_test_split(merged_df, train_size=6000, stratify=merged_df[\"Good_WM_Memory\"], random_state=42)\n",
    "df_sampled[\"Good_WM_Memory\"].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/opt/notebooks/TABPFN/02_UKB/00_data/validation_data/00_National_Cohort/\", exist_ok=True)\n",
    "#load middle age control data\n",
    "command = \"dx download file-GyGf21QJ34g4gkp8zB58x916 --output /opt/notebooks/TABPFN/02_UKB/00_data/validation_data/00_National_Cohort/all_ages_all_ids_subset_middle_age.csv --overwrite\"\n",
    "subprocess.run(command, shell=True)\n",
    "#load mri data\n",
    "command = \"dx download file-GyGX3qjJ34gPZ0g99xYkBFkY --output /opt/notebooks/TABPFN/02_UKB/00_data/validation_data/00_National_Cohort/aparc.thickness_aseg.volume_aparc.volume_deconfounded_but_age.csv --overwrite\"\n",
    "subprocess.run(command, shell=True)\n",
    "label_df_control = pd.read_csv(\"../00_data/validation_data/00_National_Cohort/all_ages_all_ids_subset_middle_age.csv\")\n",
    "df_control = pd.read_csv(\"../00_data/validation_data/00_National_Cohort/aparc.thickness_aseg.volume_aparc.volume_deconfounded_but_age.csv\")\n",
    "\n",
    "label_df_control = label_df_control.rename(columns={\"label_age_group\": \"Good_WM_Memory\"})\n",
    "#set threshold for label_df_control (if it is bigger then 2 make it 1 else 0)\n",
    "label_df_control['Good_WM_Memory'] = label_df_control['Good_WM_Memory'].apply(lambda x: 0 if x > 1 else 1)\n",
    "label_df_control = label_df_control[['ID', 'Good_WM_Memory']]\n",
    "merged_df_control = pd.merge(df_control, label_df_control, on='ID', how='inner')\n",
    "merged_df_control.dropna(inplace=True)\n",
    "#sample 300 for validation with same distribution based on Good_WM_Memory\n",
    "\n",
    "merged_df_control, _ = train_test_split(merged_df_control, train_size=400, stratify=merged_df_control[\"Good_WM_Memory\"], random_state=42)\n",
    "\n",
    "X_control_source = merged_df_control.drop([\"ID\", \"Good_WM_Memory\"], axis=1)\n",
    "y_control_source = merged_df_control[\"Good_WM_Memory\"]\n",
    "merged_df_control[\"Good_WM_Memory\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.drop(['Left-Thalamus-Proper', 'Right-Thalamus-Proper', 'SupraTentorialVolNotVentVox'], axis=1, inplace=True)\n",
    "column_control = df_sampled.drop([\"ID\", \"Good_WM_Memory\"], axis=1).columns\n",
    "X_control = X_control_source[column_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance_train(y_test, y_pred, y_pred_proba, y_val_bin=None, multiclass=False):\n",
    "    # Compute basic metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Random comparison\n",
    "    n_classes = len(np.unique(y_test))\n",
    "    random_y_test = np.random.randint(0, n_classes, size=y_test.shape)\n",
    "    random_balanced_acc = balanced_accuracy_score(random_y_test, y_pred)\n",
    "    \n",
    "    # ROC AUC (if probabilities provided)\n",
    "    if y_val_bin is not None:\n",
    "        y_test = y_val_bin\n",
    "    if multiclass:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    else:\n",
    "        #binary classification\n",
    "        auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'random_balanced_accuracy': random_balanced_acc,\n",
    "        'classification_report': report\n",
    "    }\n",
    "    \n",
    "    if auc is not None:\n",
    "        results['roc_auc'] = auc\n",
    "    \n",
    "    return results, balanced_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(model, X_val, y_val, original_classes=None, multi_class=False):\n",
    "    if multi_class:\n",
    "        y_pred_proba = model.predict(X_val)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        #print(y_pred)\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_val)\n",
    "        y_pred = model.predict(X_val)\n",
    "        #print(y_pred)\n",
    "    \n",
    "    # Get unique classes present in validation data\n",
    "    present_classes = np.unique(y_val)\n",
    "    \n",
    "    # Get the indices of these classes in the original prediction probabilities\n",
    "    class_indices = [np.where(original_classes == cls)[0][0] for cls in present_classes]\n",
    "    \n",
    "    # Select only the probability columns for present classes\n",
    "    y_pred_proba_filtered = y_pred_proba[:, class_indices]\n",
    "    \n",
    "    # Binarize the true labels using only the present classes\n",
    "    y_val_bin = label_binarize(y_val, classes=present_classes)\n",
    "\n",
    "    results, balanced_acc = evaluate_model_performance_train(y_val, y_pred, y_pred_proba_filtered, y_val_bin)\n",
    "    print_model_performance(results)\n",
    "    return results, balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percentage_of_the_data = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "#percentage_of_the_data = [0.01]\n",
    "data = []\n",
    "percentage_dict = {}\n",
    "best_mse_mlp = float('inf')\n",
    "best_mse_lgb = float('inf')\n",
    "best_mse_tab = float('inf')\n",
    "deconfounding_strategies = [\"Nothing\", \"BE\", \"Correlation_in_Feature\",\"Correlation_with_target\", \"PCA\"]\n",
    "for percentage in percentage_of_the_data:\n",
    "        percentage_dict[percentage] = {}\n",
    "        for deconfounding_strategy in deconfounding_strategies:\n",
    "                print(f\"\\n=== Deconfounding Strategy: {deconfounding_strategy} ===\")\n",
    "                if percentage == 1:\n",
    "                        print(f\"\\n #### TRAINING WITH {percentage} OF THE DATA ####\")\n",
    "                        df_sampled_subset = df_sampled\n",
    "                else:\n",
    "                        print(f\"\\n #### TRAINING WITH {percentage} OF THE DATA ####\")\n",
    "                        df_sampled_subset, _ = train_test_split(\n",
    "                        df_sampled,\n",
    "                        train_size=percentage,  # Use train_size to get desired percentage\n",
    "                        stratify=df_sampled[\"Good_WM_Memory\"],\n",
    "                        random_state=42\n",
    "                        )\n",
    "\n",
    "                y = df_sampled_subset[\"Good_WM_Memory\"]\n",
    "                X = df_sampled_subset.drop([\"ID\", \"Good_WM_Memory\"], axis=1)\n",
    "\n",
    "                print(f\"Training data shape: {X.shape}, length of y: {len(y)}\")\n",
    "                print(f\"Training data class distribution: {y.value_counts()}\")\n",
    "                \n",
    "\n",
    "                skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                cv_results = {\n",
    "                        'accuracy': [],\n",
    "                        'balanced_accuracy': [],\n",
    "                        'roc_auc': [],\n",
    "                        'classification_reports': []\n",
    "                }\n",
    "\n",
    "                tabpfn_results = []\n",
    "                tabpfn_results_eval = []\n",
    "                lgb_results = []\n",
    "                lgb_results_eval = []\n",
    "                random_results = []\n",
    "                mlp_results = []\n",
    "                mlp_results_eval = []\n",
    "                cnn_results = []\n",
    "                cnn_results_eval = []\n",
    "                model_dict = {}\n",
    "                model_results = {}\n",
    "\n",
    "\n",
    "                best_balanced_accuracy_mlp = 0\n",
    "                best_balanced_accuracy_tabpfn = 0\n",
    "                best_balanced_accuracy_lgb = 0\n",
    "                for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
    "                        unique_classes = np.unique(y)\n",
    "                        missing_classes = [cls for cls in unique_classes if cls not in y.iloc[val_index]]\n",
    "                        for cls in missing_classes:\n",
    "                                cls_indices = np.where(y == cls)[0]  # Get all indices of the missing class\n",
    "                                # Check if removing a sample would leave train set empty for the class\n",
    "                                train_cls_indices = np.intersect1d(cls_indices, train_index)\n",
    "\n",
    "                                if len(train_cls_indices) <= 1:\n",
    "                                        # If moving the last one, instead take a duplicate from the whole y array\n",
    "                                        cls_idx_to_move = np.random.choice(cls_indices, 1)[0]\n",
    "                                else:\n",
    "                                        cls_idx_to_move = np.random.choice(train_cls_indices, 1)[0]\n",
    "                                # Add to validation set\n",
    "                                val_index = np.append(val_index, cls_idx_to_move)\n",
    "                                # Remove only if it's not the last one in train\n",
    "                                if len(train_cls_indices) > 1:\n",
    "                                        train_index = np.setdiff1d(train_index, cls_idx_to_move)\n",
    "                        print(f\"\\nFold {fold}\")\n",
    "                        X_train, X_test = X.iloc[train_index], X.iloc[val_index]\n",
    "                        y_train, y_test = y.iloc[train_index], y.iloc[val_index]\n",
    "                        X_control = X_control_source.copy()\n",
    "                        y_control = y_control_source.copy()\n",
    "\n",
    "                        #check if columns between control and trai nare the same\n",
    "                        try:\n",
    "                                column_control = X_train.columns\n",
    "                                X_control = X_control[column_control]\n",
    "                        except Exception as e:\n",
    "                                print(\"Columns are not the same\")\n",
    "                                print(e)\n",
    "                        #scaler = MinMaxScaler()\n",
    "                        df_columns = X.columns\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train_scaled = scaler.fit_transform(X_train)\n",
    "                        X_test_scaled = scaler.transform(X_test)\n",
    "                        X_control_scaled = scaler.fit_transform(X_control)\n",
    "                        if deconfounding_strategy == \"BE\":\n",
    "                                X_train, X_test, X_control = feature_extration_with_BE(X_train_scaled, X_test_scaled, X_control_scaled, y_train, df_columns=df_columns)\n",
    "                        elif deconfounding_strategy == \"PCA\":\n",
    "                                X_train, X_test, X_control= feature_extration_with_PCA(X_train_scaled, X_test_scaled, X_control_scaled,  n_components=50)\n",
    "                        elif deconfounding_strategy == \"Correlation_in_Feature\":\n",
    "                                X_train, X_test, X_control = feature_extraction_with_Pearson(X_train_scaled, X_test_scaled, X_control_scaled, y_train, threshold=0.6, df_columns=df_columns)\n",
    "                        elif deconfounding_strategy == \"Correlation_with_target\":\n",
    "                                X_train, X_test, X_control = feature_extraction_best_corr_with_target(X_train_scaled, X_test_scaled, X_control_scaled, y_train, threshold=0.6, df_columns=df_columns)\n",
    "                        elif deconfounding_strategy == \"Nothing\":\n",
    "                                X_train, X_test, X_control = X_train_scaled, X_test_scaled, X_control_scaled\n",
    "                        n_classes = len(np.unique(y_test))\n",
    "                        random_y_test = np.random.randint(0, n_classes, size=y_test.shape)\n",
    "                        random_y_pred_proba = np.random.rand(len(y_test), n_classes)\n",
    "                        random_y_pred_proba /= random_y_pred_proba.sum(axis=1)[:, np.newaxis]\n",
    "                        results, balanced_accuracy  = evaluate_model_performance_train(y_test, random_y_test, random_y_pred_proba)\n",
    "                        print(\"RANDOM PERFORMANCE\")\n",
    "                        print_model_performance(results)\n",
    "                        random_results.append(results)\n",
    "\n",
    "                        cnnclf = create_cnn_model(input_shape=(X_train.shape[1], 1), num_classes=len(y.unique()))\n",
    "                        X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "                        X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "                        X_control_cnn = X_control.reshape(X_control.shape[0], X_control.shape[1], 1)\n",
    "                        cnnclf.fit(X_train_cnn, pd.get_dummies(y_train), epochs=10, batch_size=32, verbose=0)\n",
    "                        y_pred_proba = cnnclf.predict(X_test_cnn)\n",
    "                        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "                        results, balanced_accuracy = evaluate_model_performance_train(y_test, y_pred, y_pred_proba)\n",
    "                        print(\"CNN PERFORMANCE\")\n",
    "                        print_model_performance(results)\n",
    "                        cnn_results.append(results)\n",
    "                        print(\"CNN PERFORMANCE FOR CONTROL\")\n",
    "                        results, balanced_accuracy = predict_and_evaluate(cnnclf, X_control_cnn, y_control, original_classes = np.unique(y_train), multi_class=True)\n",
    "                        cnn_results_eval.append(results)\n",
    "                        clean_up_cuda(cnnclf)\n",
    "\n",
    "                        mlpclf = create_mlp_model(input_shape=X_train.shape[1], num_classes=len(y.unique()))\n",
    "                        mlpclf.fit(X_train, pd.get_dummies(y_train), epochs=10, batch_size=32, verbose=0)\n",
    "                        y_pred_proba = mlpclf.predict(X_test)\n",
    "                        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "                        results, balanced_accuracy = evaluate_model_performance_train(y_test, y_pred, y_pred_proba)\n",
    "                        print(\"MLP PERFORMANCE\")\n",
    "                        print_model_performance(results)\n",
    "                        mlp_results.append(results)\n",
    "                        #model_dict[\"mlp\"] = mlpclf\n",
    "                        print(\"MLP PERFORMANCE FOR CONTROL\")\n",
    "                        results, balanced_accuracy = predict_and_evaluate(mlpclf, X_control, y_control, original_classes = np.unique(y_train), multi_class=True)\n",
    "                        mlp_results_eval.append(results)\n",
    "                        if balanced_accuracy > best_balanced_accuracy_mlp:\n",
    "                                best_balanced_accuracy_mlp = balanced_accuracy\n",
    "                                model_dict[\"mlp\"] = mlpclf\n",
    "                        clean_up_cuda(mlpclf)\n",
    "\n",
    "                        tabclf = TabPFNClassifier()\n",
    "                        tabclf.fit(X_train, y_train)\n",
    "                        y_pred_proba = tabclf.predict_proba(X_test)\n",
    "                        y_pred = tabclf.predict(X_test)\n",
    "                        results, balanced_accuracy = evaluate_model_performance_train(y_test, y_pred, y_pred_proba)\n",
    "                        print(\"tabpfn PERFORMANCE\")\n",
    "                        print_model_performance(results)\n",
    "                        tabpfn_results.append(results)\n",
    "                        #model_dict[\"tabpfn\"] = tabclf\n",
    "                        original_classes = tabclf.classes_\n",
    "                        print(\"tabpfn PERFORMANCE FOR CONTROL\")\n",
    "                        results, balanced_accuracy = predict_and_evaluate(tabclf, X_control, y_control, original_classes=original_classes)\n",
    "                        tabpfn_results_eval.append(results)\n",
    "                        if balanced_accuracy > best_balanced_accuracy_tabpfn:\n",
    "                                best_balanced_accuracy_tabpfn = balanced_accuracy\n",
    "                                model_dict[\"tabpfn\"] = tabclf\n",
    "                        clean_up_cuda(tabclf)\n",
    "                        \n",
    "                        lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "                        lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "                        params = {\n",
    "                        'objective': 'multiclass',\n",
    "                        'num_class': len(y.unique()),\n",
    "                        'metric': 'multi_logloss',\n",
    "                        'num_leaves': 31,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'feature_fraction': 0.9,\n",
    "                        'seed': 42,\n",
    "                        'verbose': -1\n",
    "                        }\n",
    "                        lgbclf = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], num_boost_round=1000)\n",
    "                        y_pred_proba = lgbclf.predict(X_test)\n",
    "                        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "                        results, balanced_accuracy = evaluate_model_performance_train(y_test, y_pred, y_pred_proba)\n",
    "                        print(\"LGBM PERFORMANCE\")\n",
    "                        print_model_performance(results)\n",
    "                        lgb_results.append(results)\n",
    "                        print(\"LGBM PERFORMANCE FOR CONTROL\")\n",
    "                        results, balanced_accuracy = predict_and_evaluate(lgbclf, X_control, y_control, original_classes=original_classes, multi_class=True)\n",
    "                        lgb_results_eval.append(results)\n",
    "                        if balanced_accuracy > best_balanced_accuracy_lgb:\n",
    "                                best_balanced_accuracy_lgb = balanced_accuracy\n",
    "                                model_dict[\"lgb\"] = lgbclf\n",
    "                        clean_up_cuda(lgbclf)\n",
    "\n",
    "                random_summary = aggregate_cv_metrics_and_print(random_results, \"Random\")\n",
    "                tabpfn_summary = aggregate_cv_metrics_and_print(tabpfn_results, \"TabPFN\")\n",
    "                lgb_summary = aggregate_cv_metrics_and_print(lgb_results, \"LGBM\")\n",
    "                mlp_summary = aggregate_cv_metrics_and_print(mlp_results, \"MLP\")\n",
    "                cnn_summary = aggregate_cv_metrics_and_print(cnn_results, \"CNN\")\n",
    "\n",
    "                tabpfn_eval_summary = aggregate_cv_metrics_and_print(tabpfn_results_eval, \"TabPFN\", \"Control\")\n",
    "                lgb_eval_summary = aggregate_cv_metrics_and_print(lgb_results_eval, \"LGBM\", \"Control\")\n",
    "                mlp_eval_summary = aggregate_cv_metrics_and_print(mlp_results_eval, \"MLP\", \"Control\")\n",
    "                cnn_eval_summary = aggregate_cv_metrics_and_print(cnn_results_eval, \"CNN\", \"Control\")\n",
    "\n",
    "                percentage_dict[percentage][deconfounding_strategy] = {\n",
    "                \"TabPFN\": {\n",
    "                        \"results\": tabpfn_summary,\n",
    "                        \"results_eval\": tabpfn_eval_summary,\n",
    "                        \"cv_results\": tabpfn_results,\n",
    "                        \"cv_results_eval\": tabpfn_results_eval\n",
    "                },\n",
    "                \"LGBM\": {\n",
    "                        \"results\": lgb_summary,\n",
    "                        \"results_eval\": lgb_eval_summary,\n",
    "                        \"cv_results\": lgb_results,\n",
    "                        \"cv_results_eval\": lgb_results_eval\n",
    "                },\n",
    "                \"Random\": {\n",
    "                        \"results\": random_summary,\n",
    "                        \"results_eval\": random_summary,\n",
    "                        \"cv_results\": random_results,\n",
    "                        \"cv_results_eval\": random_results,\n",
    "\n",
    "                },\n",
    "                \"MLP\": {\n",
    "                        \"results\": mlp_summary,\n",
    "                        \"results_eval\": mlp_eval_summary,\n",
    "                        \"cv_results\": mlp_results,\n",
    "                        \"cv_results_eval\": mlp_results_eval\n",
    "                },\n",
    "                \"CNN\": {\n",
    "                        \"results\": cnn_summary,\n",
    "                        \"results_eval\": cnn_eval_summary,\n",
    "                        \"cv_results\": cnn_results,\n",
    "                        \"cv_results_eval\": cnn_results_eval\n",
    "                }\n",
    "            }\n",
    "        Feature_extraction_applied = False\n",
    "        Pretraining_applied = False\n",
    "        all_rows = []\n",
    "        log_file = \"/opt/notebooks/results_classification.csv\"\n",
    "\n",
    "        # Iterate over percentages and their associated models\n",
    "        for percentage, models in percentage_dict.items():\n",
    "            for feature_extraction, feature_summary_dict in models.items():\n",
    "                for model_name, train_summary in feature_summary_dict.items():\n",
    "                    for i, (cv_result, cv_result_eval) in enumerate(zip(train_summary[\"cv_results\"], train_summary[\"cv_results_eval\"])):\n",
    "                        # Prepare training row\n",
    "                        row_train = {\n",
    "                            \"label_col\": label_col,\n",
    "                            \"mri_table\": mri_table,\n",
    "                            \"test_set_size\": f\"{(1 - percentage):.2%} (approx. of data left for test)\",\n",
    "                            \"Feature_extraction_applied\": Feature_extraction_applied,\n",
    "                            \"Pretraining_applied\": Pretraining_applied,\n",
    "                            \"model_type\": model_name,\n",
    "                            \"Accuracy\": cv_result.get(\"accuracy\", None),\n",
    "                            \"AUC\": cv_result.get(\"roc_auc\", None),  # Adjust the key if your aggregator uses a different one\n",
    "                            \"Balanced_ACC\": cv_result.get(\"balanced_accuracy\", None),\n",
    "                            \"Permutation_Balanced_ACC\": cv_result.get(\"random_balanced_accuracy\", None),\n",
    "                            \"number_of_cross_validations\": n_splits,\n",
    "                            \"cross_validation_count\": i,\n",
    "                            \"search_term\": f\"{percentage}_{feature_extraction}_{model_name}_train\",\n",
    "                            \"percentage_of_data\": percentage,  # Storing the used fraction\n",
    "                            \"eval_or_train\": \"train\"\n",
    "                        }\n",
    "\n",
    "                        # Prepare evaluation row\n",
    "                        row_eval = {\n",
    "                            \"label_col\": label_col,\n",
    "                            \"mri_table\": mri_table,\n",
    "                            \"test_set_size\": f\"{(1 - percentage):.2%} (approx. of data left for test)\",\n",
    "                            \"Feature_extraction_applied\": Feature_extraction_applied,\n",
    "                            \"Pretraining_applied\": Pretraining_applied,\n",
    "                            \"model_type\": model_name,\n",
    "                            \"Accuracy\": cv_result_eval.get(\"accuracy\", None),\n",
    "                            \"AUC\": cv_result_eval.get(\"roc_auc\", None),  # Adjust the key if your aggregator uses a different one\n",
    "                            \"Balanced_ACC\": cv_result_eval.get(\"balanced_accuracy\", None),\n",
    "                            \"Permutation_Balanced_ACC\": cv_result_eval.get(\"random_balanced_accuracy\", None),\n",
    "                            \"number_of_cross_validations\": n_splits,\n",
    "                            \"cross_validation_count\": i,\n",
    "                            \"search_term\": f\"{percentage}_{feature_extraction}_{model_name}_eval\",\n",
    "                            \"percentage_of_data\": percentage,  # Storing the used fraction\n",
    "                            \"eval_or_train\": \"eval\"\n",
    "                        }\n",
    "\n",
    "                        # Append both rows to the main list\n",
    "                        all_rows.append(row_train)\n",
    "                        all_rows.append(row_eval)\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df_results = pd.DataFrame(all_rows)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df_results.to_csv(log_file, index=False)\n",
    "        logs_path = \"project-GqzxkVQJ34g6ygFJ4ZbvqBYF:/Esra/00_CLIP/01_training_logs/\"\n",
    "        label = os.environ.get(\"DX_JOB_ID\") \n",
    "        logs_path_label = os.path.join(logs_path, label)\n",
    "        dx_mkdir_command = f\"dx mkdir '{logs_path_label}'\"\n",
    "        subprocess.run(dx_mkdir_command, shell=True)\n",
    "        time_tag = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        command_csv = f\"dx upload '{log_file}' --path '{logs_path_label}/{time_tag}_result_tabpfn.csv'\"\n",
    "        subprocess.run(command_csv, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_instance():\n",
    "    job_id = os.environ.get(\"DX_JOB_ID\")\n",
    "    if job_id:\n",
    "        print(f\"Terminating job: {job_id}\")\n",
    "        # Terminate the job using dx terminate\n",
    "        subprocess.run([\"dx\", \"terminate\", job_id], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminate_instance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
