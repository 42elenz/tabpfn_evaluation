{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 01:46:16.067595: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-24 01:46:16.082876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737679576.102342 2208547 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737679576.108315 2208547 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-24 01:46:16.128949: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keras imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Scikeras wrapper for Keras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# (Hypothetical) TabPFN Regressor\n",
    "# If the TabPFN package does not provide a regressor, remove or replace this import\n",
    "from tabpfn import TabPFNRegressor  # Placeholder for a potential TabPFNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############################################################################\n",
    "# MLP Model Definition\n",
    "###############################################################################\n",
    "def create_mlp_model(input_shape):\n",
    "    \"\"\"\n",
    "    Create a simple MLP model for regression.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(1024, activation=\"relu\", input_shape=(input_shape,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        # Final layer for regression: linear activation, 1 output\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# CUDA / Memory Cleanup\n",
    "###############################################################################\n",
    "def clean_up_cuda(model):\n",
    "    \"\"\"\n",
    "    Free up GPU memory and clear Keras session.\n",
    "    \"\"\"\n",
    "    # Delete the Keras model\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    \n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Free CUDA memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    \n",
    "    print(\"CUDA memory cleared and model deleted.\")\n",
    "\n",
    "###############################################################################\n",
    "# Regression Metrics & Aggregation\n",
    "###############################################################################\n",
    "def evaluate_regression_performance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute regression metrics for predictions.\n",
    "    Returns a dictionary with MSE, MAE, and R2.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def print_regression_performance(results):\n",
    "    \"\"\"\n",
    "    Print regression performance metrics nicely.\n",
    "    \"\"\"\n",
    "    print(f\"MSE: {results['mse']:.4f}\")\n",
    "    print(f\"MAE: {results['mae']:.4f}\")\n",
    "    print(f\"R²:  {results['r2']:.4f}\")\n",
    "\n",
    "def aggregate_cv_metrics(all_results):\n",
    "    \"\"\"\n",
    "    Aggregate cross-validation metrics (MSE, MAE, R2)\n",
    "    and return mean + std across folds.\n",
    "    \"\"\"\n",
    "    aggregated = {\n",
    "        'mse': [],\n",
    "        'mae': [],\n",
    "        'r2': []\n",
    "    }\n",
    "    \n",
    "    for result in all_results:\n",
    "        aggregated['mse'].append(result['mse'])\n",
    "        aggregated['mae'].append(result['mae'])\n",
    "        aggregated['r2'].append(result['r2'])\n",
    "        \n",
    "    summary = {\n",
    "        'mean_mse':  np.mean(aggregated['mse']),\n",
    "        'std_mse':   np.std(aggregated['mse']),\n",
    "        'mean_mae':  np.mean(aggregated['mae']),\n",
    "        'std_mae':   np.std(aggregated['mae']),\n",
    "        'mean_r2':   np.mean(aggregated['r2']),\n",
    "        'std_r2':    np.std(aggregated['r2']),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "def print_cv_summary(summary):\n",
    "    \"\"\"\n",
    "    Print the aggregated CV summary (MSE, MAE, R²).\n",
    "    \"\"\"\n",
    "    print(f\"Mean MSE:  {summary['mean_mse']:.4f} ± {summary['std_mse']:.4f}\")\n",
    "    print(f\"Mean MAE:  {summary['mean_mae']:.4f} ± {summary['std_mae']:.4f}\")\n",
    "    print(f\"Mean R²:   {summary['mean_r2']:.4f} ± {summary['std_r2']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/00_NAKO/00_data/deconfounded_but_age/aparc.thickness_aseg.volume_aparc.volume.csv\")\n",
    "label_df = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/00_NAKO/00_data/age_label/all_ages_all_ids_healthy.csv\")\n",
    "n_splits = 5\n",
    "\n",
    "merged_df = pd.merge(df, label_df, on='ID', how='inner')\n",
    "merged_df.dropna(inplace=True)\n",
    "df_sampled, _ = train_test_split(merged_df, train_size=10000, stratify=merged_df[\"label_age_group\"], random_state=42)\n",
    "df_sampled[\"label_Age\"].value_counts()\n",
    "\n",
    "y = df_sampled[\"label_Age\"]\n",
    "col_to_drop = [col for col in label_df.columns]\n",
    "X = df_sampled.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Sex', 'label_Age', 'Site', 'label_age_group']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_Age\n",
       "23.00    30\n",
       "25.00    28\n",
       "21.00    28\n",
       "22.00    23\n",
       "24.00    17\n",
       "40.00    13\n",
       "26.00    13\n",
       "20.75    13\n",
       "32.00    11\n",
       "48.00    11\n",
       "21.75    11\n",
       "20.25    11\n",
       "20.50    11\n",
       "29.00    11\n",
       "24.25    10\n",
       "22.75    10\n",
       "22.50    10\n",
       "22.25    10\n",
       "49.00     9\n",
       "23.75     9\n",
       "37.00     9\n",
       "19.75     8\n",
       "21.50     8\n",
       "47.00     8\n",
       "19.00     8\n",
       "23.25     8\n",
       "24.75     7\n",
       "30.00     7\n",
       "31.00     7\n",
       "43.00     7\n",
       "50.00     7\n",
       "41.00     7\n",
       "23.50     6\n",
       "36.00     6\n",
       "21.25     6\n",
       "45.00     6\n",
       "27.00     6\n",
       "33.00     6\n",
       "42.00     6\n",
       "20.00     6\n",
       "46.00     5\n",
       "38.00     5\n",
       "28.00     5\n",
       "19.25     5\n",
       "39.00     4\n",
       "44.00     4\n",
       "35.00     3\n",
       "34.00     3\n",
       "25.50     3\n",
       "18.25     3\n",
       "18.75     3\n",
       "19.50     2\n",
       "25.25     2\n",
       "24.50     2\n",
       "25.75     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_control = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/01_Validation_data_set/00_data/final_folder/aparc.thickness_aparc.volume_aseg.volume.csv\")\n",
    "label_df_control = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/01_Validation_data_set/00_data/final_folder/aparc.thickness_aparc.volume_aseg.volume_label.csv\")\n",
    "\n",
    "label_df_control = label_df_control[['ID', 'label_Age']]\n",
    "df_control = df_control[df.columns]\n",
    "merged_df_control = pd.merge(df_control, label_df_control, on='ID', how='inner')\n",
    "merged_df_control.dropna(inplace=True)\n",
    "\n",
    "X_control = merged_df_control.drop([\"ID\", \"label_Age\"], axis=1)\n",
    "y_control = merged_df_control[\"label_Age\"]\n",
    "\n",
    "merged_df_control[\"label_Age\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "478 478\n",
      "192 192\n"
     ]
    }
   ],
   "source": [
    "#check len of X and y\n",
    "print(len(X), len(y))\n",
    "print(len(X_control), len(y_control))\n",
    "#columns number\n",
    "print(X.shape[1], X_control.shape[1])\n",
    "\n",
    "for col in X.columns:\n",
    "    if col not in X_control.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Random Baseline Performance:\n",
      "MSE: 320.2751\n",
      "MAE: 14.4533\n",
      "R²:  -1.0360\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 79.8211\n",
      "MAE: 6.7917\n",
      "R²:  0.4926\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 386.5899\n",
      "MAE: 17.6893\n",
      "R²:  -3.8075\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 36.0748\n",
      "MAE: 4.7365\n",
      "R²:  0.7707\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 532.5100\n",
      "MAE: 20.4882\n",
      "R²:  -5.6221\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47482\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 48.651500\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 42.7030\n",
      "MAE: 5.1975\n",
      "R²:  0.7285\n",
      "LightGBM Performance on Control:\n",
      "MSE: 536.4645\n",
      "MAE: 21.0824\n",
      "R²:  -5.6713\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "=== Fold 2 ===\n",
      "Random Baseline Performance:\n",
      "MSE: 321.1938\n",
      "MAE: 14.3696\n",
      "R²:  -1.1102\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 61.1832\n",
      "MAE: 6.1776\n",
      "R²:  0.5980\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 595.5320\n",
      "MAE: 22.7579\n",
      "R²:  -6.4058\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 34.6817\n",
      "MAE: 4.6104\n",
      "R²:  0.7722\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 519.0193\n",
      "MAE: 20.2498\n",
      "R²:  -5.4543\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47482\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 48.721750\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 41.6806\n",
      "MAE: 5.1222\n",
      "R²:  0.7262\n",
      "LightGBM Performance on Control:\n",
      "MSE: 535.4434\n",
      "MAE: 20.9587\n",
      "R²:  -5.6586\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "=== Fold 3 ===\n",
      "Random Baseline Performance:\n",
      "MSE: 305.1514\n",
      "MAE: 13.9811\n",
      "R²:  -1.0446\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 53.7741\n",
      "MAE: 5.8145\n",
      "R²:  0.6397\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 498.3438\n",
      "MAE: 20.7369\n",
      "R²:  -5.1972\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 32.7352\n",
      "MAE: 4.4880\n",
      "R²:  0.7807\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 516.4748\n",
      "MAE: 20.3301\n",
      "R²:  -5.4227\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47478\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 48.576125\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 39.7429\n",
      "MAE: 4.9855\n",
      "R²:  0.7337\n",
      "LightGBM Performance on Control:\n",
      "MSE: 541.1108\n",
      "MAE: 21.2358\n",
      "R²:  -5.7291\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "=== Fold 4 ===\n",
      "Random Baseline Performance:\n",
      "MSE: 308.4111\n",
      "MAE: 13.9839\n",
      "R²:  -1.0222\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 59.1790\n",
      "MAE: 6.2124\n",
      "R²:  0.6120\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 401.9473\n",
      "MAE: 18.1921\n",
      "R²:  -3.9985\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 37.1721\n",
      "MAE: 4.7444\n",
      "R²:  0.7563\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 532.8092\n",
      "MAE: 20.5699\n",
      "R²:  -5.6258\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47481\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 48.690250\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 44.2532\n",
      "MAE: 5.2662\n",
      "R²:  0.7098\n",
      "LightGBM Performance on Control:\n",
      "MSE: 541.3388\n",
      "MAE: 21.1749\n",
      "R²:  -5.7319\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "=== Fold 5 ===\n",
      "Random Baseline Performance:\n",
      "MSE: 310.6067\n",
      "MAE: 14.2005\n",
      "R²:  -0.9980\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 64.3164\n",
      "MAE: 6.4347\n",
      "R²:  0.5863\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 404.0842\n",
      "MAE: 18.2118\n",
      "R²:  -4.0250\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 34.4355\n",
      "MAE: 4.5907\n",
      "R²:  0.7785\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 529.0818\n",
      "MAE: 20.4185\n",
      "R²:  -5.5795\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47478\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 48.690375\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 41.4670\n",
      "MAE: 5.0748\n",
      "R²:  0.7333\n",
      "LightGBM Performance on Control:\n",
      "MSE: 537.9237\n",
      "MAE: 21.1353\n",
      "R²:  -5.6894\n",
      "CUDA memory cleared and model deleted.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# K-Fold Cross-Validation Setup\n",
    "###############################################################################\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "###############################################################################\n",
    "# Training & Evaluation\n",
    "###############################################################################\n",
    "mlp_results = []\n",
    "lgb_results = []\n",
    "tabpfn_results = []\n",
    "random_results = []\n",
    "\n",
    "mlp_results_eval = []\n",
    "lgb_results_eval = []\n",
    "tabpfn_results_eval = []\n",
    "\n",
    "model_dict = {}\n",
    "best_mse_mlp = float('inf')\n",
    "best_mse_lgb = float('inf')\n",
    "best_mse_tab = float('inf')\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "    X_control_scaled = scaler.fit_transform(X_control)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Random Baseline\n",
    "    # -------------------------------\n",
    "    # We'll generate random predictions from a normal distribution \n",
    "    # matching the train target's mean and std\n",
    "    random_predictions = np.random.normal(loc=y_train.mean(), scale=y_train.std(), size=len(y_val))\n",
    "    random_perf = evaluate_regression_performance(y_val, random_predictions)\n",
    "    print(\"Random Baseline Performance:\")\n",
    "    print_regression_performance(random_perf)\n",
    "    random_results.append(random_perf)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # MLP\n",
    "    # -------------------------------\n",
    "    # KerasRegressor or direct model\n",
    "    mlp_model = create_mlp_model(input_shape=X_train_scaled.shape[1])\n",
    "    mlp_model.fit(X_train_scaled, y_train, \n",
    "                  epochs=10, \n",
    "                  batch_size=32,\n",
    "                  verbose=0)\n",
    "    \n",
    "    y_pred_mlp = mlp_model.predict(X_val_scaled).ravel()  # ensure shape (n,)\n",
    "    mlp_perf = evaluate_regression_performance(y_val, y_pred_mlp)\n",
    "    print(\"\\nMLP Performance on Validation:\")\n",
    "    print_regression_performance(mlp_perf)\n",
    "    mlp_results.append(mlp_perf)\n",
    "    \n",
    "    # Evaluate on control data\n",
    "    y_pred_mlp_ctrl = mlp_model.predict(X_control_scaled).ravel()\n",
    "    mlp_perf_ctrl = evaluate_regression_performance(y_control, y_pred_mlp_ctrl)\n",
    "    print(\"MLP Performance on Control:\")\n",
    "    print_regression_performance(mlp_perf_ctrl)\n",
    "    mlp_results_eval.append(mlp_perf_ctrl)\n",
    "    \n",
    "    # Keep best MLP model based on MSE\n",
    "    if mlp_perf['mse'] < best_mse_mlp:\n",
    "        best_mse_mlp = mlp_perf['mse']\n",
    "        model_dict[\"mlp\"] = mlp_model\n",
    "    \n",
    "    # Clean up\n",
    "    clean_up_cuda(mlp_model)\n",
    "\n",
    "    # -------------------------------\n",
    "    # (Hypothetical) TabPFN Regressor\n",
    "    # -------------------------------\n",
    "    # NOTE: If TabPFNClassifier is the only option, you must skip or replace this.\n",
    "    try:\n",
    "        tabclf = TabPFNRegressor()  # Ideally TabPFNRegressor() if available\n",
    "        tabclf.fit(X_train_scaled, y_train)\n",
    "        y_pred_tab = tabclf.predict(X_val_scaled)  # For regression, this should be continuous\n",
    "        tab_perf = evaluate_regression_performance(y_val, y_pred_tab)\n",
    "        print(\"\\nTabPFN Regressor Performance on Validation:\")\n",
    "        print_regression_performance(tab_perf)\n",
    "        tabpfn_results.append(tab_perf)\n",
    "        \n",
    "        # Evaluate on control data\n",
    "        y_pred_tab_ctrl = tabclf.predict(X_control_scaled)\n",
    "        tab_perf_ctrl = evaluate_regression_performance(y_control, y_pred_tab_ctrl)\n",
    "        print(\"TabPFN Regressor Performance on Control:\")\n",
    "        print_regression_performance(tab_perf_ctrl)\n",
    "        tabpfn_results_eval.append(tab_perf_ctrl)\n",
    "        \n",
    "        if tab_perf['mse'] < best_mse_tab:\n",
    "            best_mse_tab = tab_perf['mse']\n",
    "            model_dict[\"tabpfn\"] = tabclf\n",
    "        \n",
    "        clean_up_cuda(tabclf)\n",
    "    except Exception as e:\n",
    "        print(\"TabPFN Regressor not available or failed. Skipping...\")\n",
    "        print(e)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # LightGBM\n",
    "    # -------------------------------\n",
    "    lgb_train = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "    lgb_eval  = lgb.Dataset(X_val_scaled,   label=y_val, reference=lgb_train)\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    lgbclf = lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=lgb_train, \n",
    "        valid_sets=[lgb_train, lgb_eval], \n",
    "        num_boost_round=1000\n",
    "    )\n",
    "    \n",
    "    y_pred_lgb = lgbclf.predict(X_val_scaled)\n",
    "    lgb_perf = evaluate_regression_performance(y_val, y_pred_lgb)\n",
    "    print(\"\\nLightGBM Performance on Validation:\")\n",
    "    print_regression_performance(lgb_perf)\n",
    "    lgb_results.append(lgb_perf)\n",
    "    \n",
    "    # Evaluate on control data\n",
    "    y_pred_lgb_ctrl = lgbclf.predict(X_control_scaled)\n",
    "    lgb_perf_ctrl = evaluate_regression_performance(y_control, y_pred_lgb_ctrl)\n",
    "    print(\"LightGBM Performance on Control:\")\n",
    "    print_regression_performance(lgb_perf_ctrl)\n",
    "    lgb_results_eval.append(lgb_perf_ctrl)\n",
    "    \n",
    "    if lgb_perf['mse'] < best_mse_lgb:\n",
    "        best_mse_lgb = lgb_perf['mse']\n",
    "        model_dict[\"lgb\"] = lgbclf\n",
    "    \n",
    "    clean_up_cuda(lgbclf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cross-Validation Summary (Random Baseline) ===\n",
      "Mean MSE:  313.1276 ± 6.4556\n",
      "Mean MAE:  14.1977 ± 0.1936\n",
      "Mean R²:   -1.0422 ± 0.0375\n",
      "\n",
      "=== Cross-Validation Summary (MLP) ===\n",
      "Mean MSE:  63.6548 ± 8.7813\n",
      "Mean MAE:  6.2862 ± 0.3217\n",
      "Mean R²:   0.5857 ± 0.0499\n",
      "\n",
      "=== Cross-Validation Summary (TabPFN) ===\n",
      "Mean MSE:  35.0199 ± 1.5113\n",
      "Mean MAE:  4.6340 ± 0.0964\n",
      "Mean R²:   0.7716 ± 0.0086\n",
      "\n",
      "=== Cross-Validation Summary (LightGBM) ===\n",
      "Mean MSE:  41.9693 ± 1.4864\n",
      "Mean MAE:  5.1293 ± 0.0970\n",
      "Mean R²:   0.7263 ± 0.0087\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Print Cross-Validation Summaries\n",
    "###############################################################################\n",
    "print(\"\\n=== Cross-Validation Summary (Random Baseline) ===\")\n",
    "random_summary = aggregate_cv_metrics(random_results)\n",
    "print_cv_summary(random_summary)\n",
    "\n",
    "print(\"\\n=== Cross-Validation Summary (MLP) ===\")\n",
    "mlp_summary = aggregate_cv_metrics(mlp_results)\n",
    "print_cv_summary(mlp_summary)\n",
    "\n",
    "print(\"\\n=== Cross-Validation Summary (TabPFN) ===\")\n",
    "if tabpfn_results:\n",
    "    tabpfn_summary = aggregate_cv_metrics(tabpfn_results)\n",
    "    print_cv_summary(tabpfn_summary)\n",
    "else:\n",
    "    print(\"No TabPFN results recorded.\")\n",
    "\n",
    "print(\"\\n=== Cross-Validation Summary (LightGBM) ===\")\n",
    "lgb_summary = aggregate_cv_metrics(lgb_results)\n",
    "print_cv_summary(lgb_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Example: Load a saved model & evaluate on control data\n",
    "###############################################################################\n",
    "# If you have a saved regression model:\n",
    "\"\"\"\n",
    "import pickle\n",
    "save_dir = \"../98_models/\"\n",
    "with open(os.path.join(save_dir, \"best_regressor.pkl\"), \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "    # For example, if it's a LightGBM model, you can just do:\n",
    "    y_pred_control = loaded_model.predict(X_control_scaled)\n",
    "    performance_control = evaluate_regression_performance(y_control, y_pred_control)\n",
    "    print(\"\\nLoaded Model Performance on Control Data:\")\n",
    "    print_regression_performance(performance_control)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NAKO_CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
