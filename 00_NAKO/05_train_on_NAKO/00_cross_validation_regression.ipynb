{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import  statsmodels.api as sm\n",
    "\n",
    "# Keras imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Scikeras wrapper for Keras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# (Hypothetical) TabPFN Regressor\n",
    "# If the TabPFN package does not provide a regressor, remove or replace this import\n",
    "from tabpfn import TabPFNRegressor  # Placeholder for a potential TabPFNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############################################################################\n",
    "# MLP Model Definition\n",
    "###############################################################################\n",
    "def create_mlp_model(input_shape):\n",
    "    \"\"\"\n",
    "    Create a simple MLP model for regression.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(1024, activation=\"relu\", input_shape=(input_shape,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        # Final layer for regression: linear activation, 1 output\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# CUDA / Memory Cleanup\n",
    "###############################################################################\n",
    "def clean_up_cuda(model):\n",
    "    \"\"\"\n",
    "    Free up GPU memory and clear Keras session.\n",
    "    \"\"\"\n",
    "    # Delete the Keras model\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    \n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Free CUDA memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    \n",
    "    print(\"CUDA memory cleared and model deleted.\")\n",
    "\n",
    "###############################################################################\n",
    "# Regression Metrics & Aggregation\n",
    "###############################################################################\n",
    "def evaluate_regression_performance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute regression metrics for predictions.\n",
    "    Returns a dictionary with MSE, MAE, and R2.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def print_regression_performance(results):\n",
    "    \"\"\"\n",
    "    Print regression performance metrics nicely.\n",
    "    \"\"\"\n",
    "    print(f\"MSE: {results['mse']:.4f}\")\n",
    "    print(f\"MAE: {results['mae']:.4f}\")\n",
    "    print(f\"R²:  {results['r2']:.4f}\")\n",
    "\n",
    "def aggregate_cv_metrics_and_print(all_results, model_name, tag=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Aggregate cross-validation metrics (MSE, MAE, R2)\n",
    "    and return mean + std across folds.\n",
    "    \"\"\"\n",
    "    aggregated = {\n",
    "        'mse': [],\n",
    "        'mae': [],\n",
    "        'r2': []\n",
    "    }\n",
    "    \n",
    "    for result in all_results:\n",
    "        aggregated['mse'].append(result['mse'])\n",
    "        aggregated['mae'].append(result['mae'])\n",
    "        aggregated['r2'].append(result['r2'])\n",
    "        \n",
    "    summary = {\n",
    "        'mean_mse':  np.mean(aggregated['mse']),\n",
    "        'std_mse':   np.std(aggregated['mse']),\n",
    "        'mean_mae':  np.mean(aggregated['mae']),\n",
    "        'std_mae':   np.std(aggregated['mae']),\n",
    "        'mean_r2':   np.mean(aggregated['r2']),\n",
    "        'std_r2':    np.std(aggregated['r2']),\n",
    "    }\n",
    "    print(f\"\\n {model_name} Classifier Performance {tag}:\")\n",
    "    print_cv_summary(summary)\n",
    "    return summary\n",
    "\n",
    "def print_cv_summary(summary):\n",
    "    \"\"\"\n",
    "    Print the aggregated CV summary (MSE, MAE, R²).\n",
    "    \"\"\"\n",
    "    print(f\"Mean MSE:  {summary['mean_mse']:.4f} ± {summary['std_mse']:.4f}\")\n",
    "    print(f\"Mean MAE:  {summary['mean_mae']:.4f} ± {summary['std_mae']:.4f}\")\n",
    "    print(f\"Mean R²:   {summary['mean_r2']:.4f} ± {summary['std_r2']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/00_NAKO/00_data/deconfounded_but_age/aparc.thickness_aseg.volume_aparc.volume.csv\")\n",
    "label_df = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/00_NAKO/00_data/age_label/all_ages_all_ids_healthy.csv\")\n",
    "n_splits = 5\n",
    "\n",
    "merged_df = pd.merge(df, label_df, on='ID', how='inner')\n",
    "merged_df.dropna(inplace=True)\n",
    "df_sampled, _ = train_test_split(merged_df, train_size=10000, stratify=merged_df[\"label_age_group\"], random_state=42)\n",
    "df_sampled[\"label_Age\"].value_counts()\n",
    "\n",
    "y = df_sampled[\"label_Age\"]\n",
    "col_to_drop = [col for col in label_df.columns]\n",
    "X = df_sampled.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_age_group\n",
       "2.0    4655\n",
       "3.0    4323\n",
       "4.0    3265\n",
       "1.0    1786\n",
       "0.0    1621\n",
       "5.0     234\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"label_age_group\"].value_counts().sort_values(ascending=False)\n",
    "\n",
    "merged_df[\"label_age_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_age_group\n",
      "3.0    2953\n",
      "2.0    2953\n",
      "4.0    2953\n",
      "1.0    1786\n",
      "0.0    1621\n",
      "5.0     234\n",
      "Name: count, dtype: int64\n",
      "Total samples: 12500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Get class distribution\n",
    "class_counts = merged_df[\"label_age_group\"].value_counts()\n",
    "\n",
    "# Step 1: Retain all samples from the 3 smallest classes\n",
    "smallest_classes = class_counts.nsmallest(3).index\n",
    "df_smallest = merged_df[merged_df[\"label_age_group\"].isin(smallest_classes)]\n",
    "\n",
    "# Step 2: Calculate remaining samples needed to reach 10,000\n",
    "remaining_size = 12500- len(df_smallest)\n",
    "\n",
    "# Step 3: Select the remaining larger classes\n",
    "remaining_classes = class_counts.nlargest(len(class_counts) - 3).index\n",
    "df_remaining = merged_df[merged_df[\"label_age_group\"].isin(remaining_classes)]\n",
    "\n",
    "# Step 4: Determine the number of samples to take per remaining class proportionally\n",
    "samples_per_class = remaining_size // len(remaining_classes)\n",
    "\n",
    "# Downsample the remaining classes to fill the dataset\n",
    "balanced_majority = df_remaining.groupby(\"label_age_group\").apply(\n",
    "    lambda x: x.sample(n=min(len(x), samples_per_class), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Combine both parts to form the final balanced dataset\n",
    "balanced_df = pd.concat([df_smallest, balanced_majority])\n",
    "\n",
    "# Verify the final class distribution\n",
    "print(balanced_df[\"label_age_group\"].value_counts())\n",
    "print(\"Total samples:\", len(balanced_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Sex', 'label_Age', 'Site', 'label_age_group']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_Age\n",
       "23.00    30\n",
       "25.00    28\n",
       "21.00    28\n",
       "22.00    23\n",
       "24.00    17\n",
       "40.00    13\n",
       "26.00    13\n",
       "20.75    13\n",
       "32.00    11\n",
       "48.00    11\n",
       "21.75    11\n",
       "20.25    11\n",
       "20.50    11\n",
       "29.00    11\n",
       "24.25    10\n",
       "22.75    10\n",
       "22.50    10\n",
       "22.25    10\n",
       "49.00     9\n",
       "23.75     9\n",
       "37.00     9\n",
       "19.75     8\n",
       "21.50     8\n",
       "47.00     8\n",
       "19.00     8\n",
       "23.25     8\n",
       "24.75     7\n",
       "30.00     7\n",
       "31.00     7\n",
       "43.00     7\n",
       "50.00     7\n",
       "41.00     7\n",
       "23.50     6\n",
       "36.00     6\n",
       "21.25     6\n",
       "45.00     6\n",
       "27.00     6\n",
       "33.00     6\n",
       "42.00     6\n",
       "20.00     6\n",
       "46.00     5\n",
       "38.00     5\n",
       "28.00     5\n",
       "19.25     5\n",
       "39.00     4\n",
       "44.00     4\n",
       "35.00     3\n",
       "34.00     3\n",
       "25.50     3\n",
       "18.25     3\n",
       "18.75     3\n",
       "19.50     2\n",
       "25.25     2\n",
       "24.50     2\n",
       "25.75     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_control = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/01_Validation_data_set/00_data/final_folder/aparc.thickness_aparc.volume_aseg.volume.csv\")\n",
    "label_df_control = pd.read_csv(\"/zi/home/esra.lenz/Documents/00_HITKIP/09_TABPFN/01_Validation_data_set/00_data/final_folder/aparc.thickness_aparc.volume_aseg.volume_label.csv\")\n",
    "\n",
    "label_df_control = label_df_control[['ID', 'label_Age']]\n",
    "df_control = df_control[df.columns]\n",
    "merged_df_control = pd.merge(df_control, label_df_control, on='ID', how='inner')\n",
    "merged_df_control.dropna(inplace=True)\n",
    "\n",
    "X_control = merged_df_control.drop([\"ID\", \"label_Age\"], axis=1)\n",
    "y_control = merged_df_control[\"label_Age\"]\n",
    "\n",
    "merged_df_control[\"label_Age\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "478 478\n",
      "192 192\n"
     ]
    }
   ],
   "source": [
    "#make sure that the order of columns is the same\n",
    "X = X[X_control.columns]\n",
    "#check len of X and y\n",
    "print(len(X), len(y))\n",
    "print(len(X_control), len(y_control))\n",
    "#columns number\n",
    "print(X.shape[1], X_control.shape[1])\n",
    "\n",
    "for col in X.columns:\n",
    "    if col not in X_control.columns:\n",
    "        print(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_with_Pearson(X, X_val, X_control, y, threshold=0.6, df_columns=None):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    correlation_matrix = X.corr().abs()\n",
    "    upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    X = X.drop(columns=to_drop)\n",
    "    X_val = X_val.drop(columns=to_drop)\n",
    "    X_control = X_control.drop(columns=to_drop)\n",
    "    return X.to_numpy(), X_val.to_numpy(), X_control.to_numpy()\n",
    "\n",
    "def feature_extration_with_PCA(X, X_val, X_control, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_control_pca = pca.transform(X_control)\n",
    "    return X_pca, X_val_pca, X_control_pca\n",
    "\n",
    "def feature_extration_with_BE(X, X_val, X_control, y, significance_level=0.05, df_columns=None):\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "        if df_columns is not None:\n",
    "            X.columns = df_columns\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        if df_columns is not None:\n",
    "            X_val.columns = df_columns\n",
    "    if isinstance(X_control, np.ndarray):\n",
    "        X_control = pd.DataFrame(X_control)\n",
    "        if df_columns is not None:\n",
    "            X_control.columns = df_columns\n",
    "    # Add constant for intercept\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    while True:\n",
    "        # Fit the OLS model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Get the p-values for each feature\n",
    "        p_values = model.pvalues\n",
    "        \n",
    "        # Find the feature with the highest p-value\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        if max_p_value > significance_level:\n",
    "            # Remove the feature with the highest p-value\n",
    "            feature_to_remove = p_values.idxmax()\n",
    "            print(f\"Removing {feature_to_remove} with p-value {max_p_value:.4f}\")\n",
    "            X = X.drop(columns=[feature_to_remove])\n",
    "            X_val = X_val.drop(columns=[feature_to_remove])\n",
    "            X_control = X_control.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "        print(\"Final Feature lengthe: \", len(X.columns))\n",
    "    # Return the final selected feature set (excluding the intercept)\n",
    "    return X.drop(columns=['const']).to_numpy(), X_val.to_numpy(), X_control.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deconfounding Strategy: BE ===\n",
      "\n",
      "=== Fold 1 ===\n",
      "Removing rh_rostralanteriorcingulate_thickness with p-value 0.9951\n",
      "Final Feature lengthe:  192\n",
      "Removing rh_supramarginal_thickness with p-value 0.9107\n",
      "Final Feature lengthe:  191\n",
      "Removing lh_posteriorcingulate_thickness with p-value 0.8544\n",
      "Final Feature lengthe:  190\n",
      "Removing lh_insula_thickness with p-value 0.8422\n",
      "Final Feature lengthe:  189\n",
      "Removing lh_transversetemporal_thickness with p-value 0.7504\n",
      "Final Feature lengthe:  188\n",
      "Removing rh_caudalanteriorcingulate_thickness with p-value 0.7101\n",
      "Final Feature lengthe:  187\n",
      "Removing rh_transversetemporal_volume with p-value 0.6295\n",
      "Final Feature lengthe:  186\n",
      "Removing Right-Amygdala with p-value 0.5995\n",
      "Final Feature lengthe:  185\n",
      "Removing lh_parsorbitalis_volume with p-value 0.5337\n",
      "Final Feature lengthe:  184\n",
      "Removing Left-vessel with p-value 0.5267\n",
      "Final Feature lengthe:  183\n",
      "Removing non-WM-hypointensities with p-value 0.4672\n",
      "Final Feature lengthe:  182\n",
      "Removing rhCerebralWhiteMatterVol with p-value 0.4634\n",
      "Final Feature lengthe:  181\n",
      "Removing rhCortexVol with p-value 0.4633\n",
      "Final Feature lengthe:  180\n",
      "Removing lhCortexVol with p-value 0.6645\n",
      "Final Feature lengthe:  179\n",
      "Removing rh_medialorbitofrontal_thickness with p-value 0.4558\n",
      "Final Feature lengthe:  178\n",
      "Removing rh_precuneus_thickness with p-value 0.4535\n",
      "Final Feature lengthe:  177\n",
      "Removing Right-WM-hypointensities with p-value 0.8079\n",
      "Final Feature lengthe:  176\n",
      "Removing rh_postcentral_thickness with p-value 0.4444\n",
      "Final Feature lengthe:  175\n",
      "Removing lh_postcentral_thickness with p-value 0.4838\n",
      "Final Feature lengthe:  174\n",
      "Removing Right-non-WM-hypointensities with p-value 0.5931\n",
      "Final Feature lengthe:  173\n",
      "Removing rh_lingual_thickness with p-value 0.4239\n",
      "Final Feature lengthe:  172\n",
      "Removing Left-WM-hypointensities with p-value 0.5084\n",
      "Final Feature lengthe:  171\n",
      "Removing rh_inferiortemporal_thickness with p-value 0.3992\n",
      "Final Feature lengthe:  170\n",
      "Removing CC_Mid_Anterior with p-value 0.3926\n",
      "Final Feature lengthe:  169\n",
      "Removing rh_bankssts_thickness with p-value 0.3925\n",
      "Final Feature lengthe:  168\n",
      "Removing Left-non-WM-hypointensities with p-value 0.7299\n",
      "Final Feature lengthe:  167\n",
      "Removing rh_transversetemporal_thickness with p-value 0.3096\n",
      "Final Feature lengthe:  166\n",
      "Removing lh_parsorbitalis_thickness with p-value 0.3101\n",
      "Final Feature lengthe:  165\n",
      "Removing lh_parahippocampal_thickness with p-value 0.2536\n",
      "Final Feature lengthe:  164\n",
      "Removing lh_pericalcarine_thickness with p-value 0.2506\n",
      "Final Feature lengthe:  163\n",
      "Removing lh_rostralmiddlefrontal_thickness with p-value 0.2126\n",
      "Final Feature lengthe:  162\n",
      "Removing rh_middletemporal_thickness with p-value 0.1931\n",
      "Final Feature lengthe:  161\n",
      "Removing rh_cuneus_thickness with p-value 0.2011\n",
      "Final Feature lengthe:  160\n",
      "Removing lh_precuneus_thickness with p-value 0.1840\n",
      "Final Feature lengthe:  159\n",
      "Removing lh_parsopercularis_thickness with p-value 0.1700\n",
      "Final Feature lengthe:  158\n",
      "Removing rh_entorhinal_thickness with p-value 0.1406\n",
      "Final Feature lengthe:  157\n",
      "Removing rh_frontalpole_volume with p-value 0.1353\n",
      "Final Feature lengthe:  156\n",
      "Removing lh_medialorbitofrontal_thickness with p-value 0.0908\n",
      "Final Feature lengthe:  155\n",
      "Removing lh_bankssts_thickness with p-value 0.0773\n",
      "Final Feature lengthe:  154\n",
      "Removing lh_transversetemporal_volume with p-value 0.0658\n",
      "Final Feature lengthe:  153\n",
      "Removing 5th-Ventricle with p-value 0.0608\n",
      "Final Feature lengthe:  152\n",
      "Removing rh_parsorbitalis_thickness with p-value 0.0537\n",
      "Final Feature lengthe:  151\n",
      "Removing rh_lateralorbitofrontal_thickness with p-value 0.1013\n",
      "Final Feature lengthe:  150\n",
      "Random Baseline Performance:\n",
      "MSE: 320.4887\n",
      "MAE: 14.3147\n",
      "R²:  -1.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1737748219.404490 2525842 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 87.8270\n",
      "MAE: 7.3057\n",
      "R²:  0.4417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 396.7838\n",
      "MAE: 18.2476\n",
      "R²:  -3.9343\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 36.9194\n",
      "MAE: 4.7797\n",
      "R²:  0.7653\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 537.4131\n",
      "MAE: 20.5205\n",
      "R²:  -5.6831\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37995\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 149\n",
      "[LightGBM] [Info] Start training from score 48.651500\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 43.5184\n",
      "MAE: 5.2785\n",
      "R²:  0.7234\n",
      "LightGBM Performance on Control:\n",
      "MSE: 548.2106\n",
      "MAE: 21.2298\n",
      "R²:  -5.8174\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      " Random Classifier Performance Validation:\n",
      "Mean MSE:  320.4887 ± 0.0000\n",
      "Mean MAE:  14.3147 ± 0.0000\n",
      "Mean R²:   -1.0374 ± 0.0000\n",
      "\n",
      " TabPFN Classifier Performance Validation:\n",
      "Mean MSE:  36.9194 ± 0.0000\n",
      "Mean MAE:  4.7797 ± 0.0000\n",
      "Mean R²:   0.7653 ± 0.0000\n",
      "\n",
      " LGBM Classifier Performance Validation:\n",
      "Mean MSE:  43.5184 ± 0.0000\n",
      "Mean MAE:  5.2785 ± 0.0000\n",
      "Mean R²:   0.7234 ± 0.0000\n",
      "\n",
      " MLP Classifier Performance Validation:\n",
      "Mean MSE:  87.8270 ± 0.0000\n",
      "Mean MAE:  7.3057 ± 0.0000\n",
      "Mean R²:   0.4417 ± 0.0000\n",
      "\n",
      " TabPFN Classifier Performance Control:\n",
      "Mean MSE:  537.4131 ± 0.0000\n",
      "Mean MAE:  20.5205 ± 0.0000\n",
      "Mean R²:   -5.6831 ± 0.0000\n",
      "\n",
      " LGBM Classifier Performance Control:\n",
      "Mean MSE:  548.2106 ± 0.0000\n",
      "Mean MAE:  21.2298 ± 0.0000\n",
      "Mean R²:   -5.8174 ± 0.0000\n",
      "\n",
      " MLP Classifier Performance Control:\n",
      "Mean MSE:  396.7838 ± 0.0000\n",
      "Mean MAE:  18.2476 ± 0.0000\n",
      "Mean R²:   -3.9343 ± 0.0000\n",
      "\n",
      "=== Fold 2 ===\n",
      "Removing rh_rostralanteriorcingulate_thickness with p-value 0.9783\n",
      "Final Feature lengthe:  192\n",
      "Removing lh_lateralorbitofrontal_volume with p-value 0.9353\n",
      "Final Feature lengthe:  191\n",
      "Removing rh_caudalanteriorcingulate_thickness with p-value 0.8101\n",
      "Final Feature lengthe:  190\n",
      "Removing rh_bankssts_thickness with p-value 0.7526\n",
      "Final Feature lengthe:  189\n",
      "Removing lh_insula_thickness with p-value 0.7442\n",
      "Final Feature lengthe:  188\n",
      "Removing lh_postcentral_thickness with p-value 0.6712\n",
      "Final Feature lengthe:  187\n",
      "Removing rh_postcentral_thickness with p-value 0.7026\n",
      "Final Feature lengthe:  186\n",
      "Removing rh_entorhinal_thickness with p-value 0.6427\n",
      "Final Feature lengthe:  185\n",
      "Removing Right-Amygdala with p-value 0.6362\n",
      "Final Feature lengthe:  184\n",
      "Removing lh_parahippocampal_volume with p-value 0.6099\n",
      "Final Feature lengthe:  183\n",
      "Removing lhCerebralWhiteMatterVol with p-value 0.5956\n",
      "Final Feature lengthe:  182\n",
      "Removing rhCerebralWhiteMatterVol with p-value 0.9096\n",
      "Final Feature lengthe:  181\n",
      "Removing Left-non-WM-hypointensities with p-value 0.5954\n",
      "Final Feature lengthe:  180\n",
      "Removing rhCortexVol with p-value 0.5954\n",
      "Final Feature lengthe:  179\n",
      "Removing 5th-Ventricle with p-value 0.5464\n",
      "Final Feature lengthe:  178\n",
      "Removing lh_precuneus_thickness with p-value 0.5405\n",
      "Final Feature lengthe:  177\n",
      "Removing lh_transversetemporal_thickness with p-value 0.4944\n",
      "Final Feature lengthe:  176\n",
      "Removing rh_inferiortemporal_thickness with p-value 0.4201\n",
      "Final Feature lengthe:  175\n",
      "Removing lh_caudalanteriorcingulate_volume with p-value 0.3927\n",
      "Final Feature lengthe:  174\n",
      "Removing lh_medialorbitofrontal_thickness with p-value 0.3580\n",
      "Final Feature lengthe:  173\n",
      "Removing rh_supramarginal_thickness with p-value 0.3101\n",
      "Final Feature lengthe:  172\n",
      "Removing lh_parsorbitalis_thickness with p-value 0.3004\n",
      "Final Feature lengthe:  171\n",
      "Removing rh_transversetemporal_volume with p-value 0.2878\n",
      "Final Feature lengthe:  170\n",
      "Removing rh_transversetemporal_thickness with p-value 0.3122\n",
      "Final Feature lengthe:  169\n",
      "Removing rh_precuneus_thickness with p-value 0.2729\n",
      "Final Feature lengthe:  168\n",
      "Removing rh_cuneus_thickness with p-value 0.2839\n",
      "Final Feature lengthe:  167\n",
      "Removing Left-vessel with p-value 0.2410\n",
      "Final Feature lengthe:  166\n",
      "Removing non-WM-hypointensities with p-value 0.2684\n",
      "Final Feature lengthe:  165\n",
      "Removing lh_pericalcarine_thickness with p-value 0.2369\n",
      "Final Feature lengthe:  164\n",
      "Removing lh_posteriorcingulate_thickness with p-value 0.1939\n",
      "Final Feature lengthe:  163\n",
      "Removing rh_insula_thickness with p-value 0.1839\n",
      "Final Feature lengthe:  162\n",
      "Removing CC_Mid_Anterior with p-value 0.1797\n",
      "Final Feature lengthe:  161\n",
      "Removing rh_superiorfrontal_thickness with p-value 0.1862\n",
      "Final Feature lengthe:  160\n",
      "Removing CerebralWhiteMatterVol with p-value 0.1617\n",
      "Final Feature lengthe:  159\n",
      "Removing lh_parahippocampal_thickness with p-value 0.1464\n",
      "Final Feature lengthe:  158\n",
      "Removing rh_lingual_thickness with p-value 0.1403\n",
      "Final Feature lengthe:  157\n",
      "Removing rh_medialorbitofrontal_thickness with p-value 0.1051\n",
      "Final Feature lengthe:  156\n",
      "Removing lh_rostralmiddlefrontal_thickness with p-value 0.1049\n",
      "Final Feature lengthe:  155\n",
      "Removing lh_inferiortemporal_thickness with p-value 0.0934\n",
      "Final Feature lengthe:  154\n",
      "Removing lh_parstriangularis_volume with p-value 0.0845\n",
      "Final Feature lengthe:  153\n",
      "Removing lh_caudalmiddlefrontal_volume with p-value 0.1268\n",
      "Final Feature lengthe:  152\n",
      "Removing lh_superiorfrontal_volume with p-value 0.3783\n",
      "Final Feature lengthe:  151\n",
      "Removing lh_middletemporal_volume with p-value 0.6051\n",
      "Final Feature lengthe:  150\n",
      "Removing lh_fusiform_volume with p-value 0.4106\n",
      "Final Feature lengthe:  149\n",
      "Removing lh_posteriorcingulate_volume with p-value 0.4193\n",
      "Final Feature lengthe:  148\n",
      "Removing lh_rostralanteriorcingulate_volume with p-value 0.2285\n",
      "Final Feature lengthe:  147\n",
      "Removing lh_paracentral_volume with p-value 0.1511\n",
      "Final Feature lengthe:  146\n",
      "Removing lh_lateraloccipital_volume with p-value 0.1234\n",
      "Final Feature lengthe:  145\n",
      "Removing lh_cuneus_volume with p-value 0.0717\n",
      "Final Feature lengthe:  144\n",
      "Removing lh_isthmuscingulate_volume with p-value 0.0797\n",
      "Final Feature lengthe:  143\n",
      "Removing lh_cuneus_thickness with p-value 0.0762\n",
      "Final Feature lengthe:  142\n",
      "Removing lh_bankssts_volume with p-value 0.0617\n",
      "Final Feature lengthe:  141\n",
      "Removing lh_entorhinal_volume with p-value 0.0586\n",
      "Final Feature lengthe:  140\n",
      "Random Baseline Performance:\n",
      "MSE: 309.6785\n",
      "MAE: 14.2486\n",
      "R²:  -1.0345\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 62.6589\n",
      "MAE: 6.3861\n",
      "R²:  0.5883\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 409.1439\n",
      "MAE: 17.9279\n",
      "R²:  -4.0880\n",
      "CUDA memory cleared and model deleted.\n",
      "TabPFN Regressor not available or failed. Skipping...\n",
      "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacty of 4.00 GiB of which 995.02 MiB is free. Process 2490994 has 161.25 MiB memory in use. Including non-PyTorch memory, this process has 2.31 GiB memory in use. Of the allocated memory 1.98 GiB is allocated by PyTorch, and 206.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34680\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 136\n",
      "[LightGBM] [Info] Start training from score 48.721750\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 41.8381\n",
      "MAE: 5.1191\n",
      "R²:  0.7251\n",
      "LightGBM Performance on Control:\n",
      "MSE: 537.7793\n",
      "MAE: 20.8834\n",
      "R²:  -5.6876\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      " Random Classifier Performance Validation:\n",
      "Mean MSE:  315.0836 ± 5.4051\n",
      "Mean MAE:  14.2817 ± 0.0331\n",
      "Mean R²:   -1.0359 ± 0.0014\n",
      "\n",
      " TabPFN Classifier Performance Validation:\n",
      "Mean MSE:  36.9194 ± 0.0000\n",
      "Mean MAE:  4.7797 ± 0.0000\n",
      "Mean R²:   0.7653 ± 0.0000\n",
      "\n",
      " LGBM Classifier Performance Validation:\n",
      "Mean MSE:  42.6782 ± 0.8402\n",
      "Mean MAE:  5.1988 ± 0.0797\n",
      "Mean R²:   0.7242 ± 0.0009\n",
      "\n",
      " MLP Classifier Performance Validation:\n",
      "Mean MSE:  75.2429 ± 12.5840\n",
      "Mean MAE:  6.8459 ± 0.4598\n",
      "Mean R²:   0.5150 ± 0.0733\n",
      "\n",
      " TabPFN Classifier Performance Control:\n",
      "Mean MSE:  537.4131 ± 0.0000\n",
      "Mean MAE:  20.5205 ± 0.0000\n",
      "Mean R²:   -5.6831 ± 0.0000\n",
      "\n",
      " LGBM Classifier Performance Control:\n",
      "Mean MSE:  542.9950 ± 5.2156\n",
      "Mean MAE:  21.0566 ± 0.1732\n",
      "Mean R²:   -5.7525 ± 0.0649\n",
      "\n",
      " MLP Classifier Performance Control:\n",
      "Mean MSE:  402.9639 ± 6.1801\n",
      "Mean MAE:  18.0877 ± 0.1598\n",
      "Mean R²:   -4.0111 ± 0.0769\n",
      "\n",
      "=== Fold 3 ===\n",
      "Removing rh_transversetemporal_volume with p-value 0.9913\n",
      "Final Feature lengthe:  192\n",
      "Removing lh_insula_thickness with p-value 0.9553\n",
      "Final Feature lengthe:  191\n",
      "Removing rh_inferiortemporal_thickness with p-value 0.9393\n",
      "Final Feature lengthe:  190\n",
      "Removing rh_transversetemporal_thickness with p-value 0.9368\n",
      "Final Feature lengthe:  189\n",
      "Removing lh_parsorbitalis_volume with p-value 0.8859\n",
      "Final Feature lengthe:  188\n",
      "Removing CerebralWhiteMatterVol with p-value 0.8839\n",
      "Final Feature lengthe:  187\n",
      "Removing rhCortexVol with p-value 0.8839\n",
      "Final Feature lengthe:  186\n",
      "Removing CC_Mid_Anterior with p-value 0.8814\n",
      "Final Feature lengthe:  185\n",
      "Removing rh_caudalanteriorcingulate_thickness with p-value 0.8326\n",
      "Final Feature lengthe:  184\n",
      "Removing Right-Amygdala with p-value 0.7517\n",
      "Final Feature lengthe:  183\n",
      "Removing lh_posteriorcingulate_thickness with p-value 0.7503\n",
      "Final Feature lengthe:  182\n",
      "Removing Right-non-WM-hypointensities with p-value 0.6745\n",
      "Final Feature lengthe:  181\n",
      "Removing rh_precuneus_thickness with p-value 0.6635\n",
      "Final Feature lengthe:  180\n",
      "Removing rh_rostralanteriorcingulate_thickness with p-value 0.5835\n",
      "Final Feature lengthe:  179\n",
      "Removing lh_postcentral_thickness with p-value 0.5498\n",
      "Final Feature lengthe:  178\n",
      "Removing rh_bankssts_thickness with p-value 0.5509\n",
      "Final Feature lengthe:  177\n",
      "Removing rh_pericalcarine_thickness with p-value 0.5395\n",
      "Final Feature lengthe:  176\n",
      "Removing Left-WM-hypointensities with p-value 0.8741\n",
      "Final Feature lengthe:  175\n",
      "Removing rh_supramarginal_thickness with p-value 0.5134\n",
      "Final Feature lengthe:  174\n",
      "Removing lh_precuneus_thickness with p-value 0.4564\n",
      "Final Feature lengthe:  173\n",
      "Removing rh_entorhinal_thickness with p-value 0.4518\n",
      "Final Feature lengthe:  172\n",
      "Removing rh_postcentral_thickness with p-value 0.4422\n",
      "Final Feature lengthe:  171\n",
      "Removing lh_parahippocampal_thickness with p-value 0.3391\n",
      "Final Feature lengthe:  170\n",
      "Removing non-WM-hypointensities with p-value 0.3383\n",
      "Final Feature lengthe:  169\n",
      "Removing lh_parahippocampal_volume with p-value 0.3475\n",
      "Final Feature lengthe:  168\n",
      "Removing lh_lateralorbitofrontal_volume with p-value 0.5792\n",
      "Final Feature lengthe:  167\n",
      "Removing rh_parsorbitalis_thickness with p-value 0.3213\n",
      "Final Feature lengthe:  166\n",
      "Removing rhCerebralWhiteMatterVol with p-value 0.2910\n",
      "Final Feature lengthe:  165\n",
      "Removing 5th-Ventricle with p-value 0.2851\n",
      "Final Feature lengthe:  164\n",
      "Removing lh_bankssts_thickness with p-value 0.2624\n",
      "Final Feature lengthe:  163\n",
      "Removing lhCerebralWhiteMatterVol with p-value 0.2554\n",
      "Final Feature lengthe:  162\n",
      "Removing Left-vessel with p-value 0.2311\n",
      "Final Feature lengthe:  161\n",
      "Removing lh_inferiortemporal_thickness with p-value 0.1748\n",
      "Final Feature lengthe:  160\n",
      "Removing rh_frontalpole_volume with p-value 0.1580\n",
      "Final Feature lengthe:  159\n",
      "Removing rh_medialorbitofrontal_thickness with p-value 0.1552\n",
      "Final Feature lengthe:  158\n",
      "Removing lh_cuneus_thickness with p-value 0.1572\n",
      "Final Feature lengthe:  157\n",
      "Removing lh_rostralmiddlefrontal_thickness with p-value 0.1529\n",
      "Final Feature lengthe:  156\n",
      "Removing rh_superiorfrontal_thickness with p-value 0.1257\n",
      "Final Feature lengthe:  155\n",
      "Removing rh_superiorparietal_thickness with p-value 0.1042\n",
      "Final Feature lengthe:  154\n",
      "Removing Right-Cerebellum-Cortex with p-value 0.1145\n",
      "Final Feature lengthe:  153\n",
      "Removing rh_inferiorparietal_thickness with p-value 0.1237\n",
      "Final Feature lengthe:  152\n",
      "Removing lh_parsorbitalis_thickness with p-value 0.0902\n",
      "Final Feature lengthe:  151\n",
      "Removing rh_parsorbitalis_volume with p-value 0.1190\n",
      "Final Feature lengthe:  150\n",
      "Removing lhCortexVol with p-value 0.1153\n",
      "Final Feature lengthe:  149\n",
      "Removing rh_lateralorbitofrontal_thickness with p-value 0.1212\n",
      "Final Feature lengthe:  148\n",
      "Removing rh_lingual_thickness with p-value 0.0776\n",
      "Final Feature lengthe:  147\n",
      "Removing lh_transversetemporal_thickness with p-value 0.0639\n",
      "Final Feature lengthe:  146\n",
      "Random Baseline Performance:\n",
      "MSE: 291.1751\n",
      "MAE: 13.7674\n",
      "R²:  -0.9510\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 52.5600\n",
      "MAE: 5.8040\n",
      "R²:  0.6478\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 436.2822\n",
      "MAE: 19.3067\n",
      "R²:  -4.4254\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 33.5715\n",
      "MAE: 4.5373\n",
      "R²:  0.7751\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 521.6410\n",
      "MAE: 20.3406\n",
      "R²:  -5.4869\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36465\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 143\n",
      "[LightGBM] [Info] Start training from score 48.576125\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 40.8312\n",
      "MAE: 5.0331\n",
      "R²:  0.7264\n",
      "LightGBM Performance on Control:\n",
      "MSE: 526.5212\n",
      "MAE: 20.8289\n",
      "R²:  -5.5476\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      " Random Classifier Performance Validation:\n",
      "Mean MSE:  307.1141 ± 12.1038\n",
      "Mean MAE:  14.1102 ± 0.2439\n",
      "Mean R²:   -1.0076 ± 0.0401\n",
      "\n",
      " TabPFN Classifier Performance Validation:\n",
      "Mean MSE:  35.2454 ± 1.6739\n",
      "Mean MAE:  4.6585 ± 0.1212\n",
      "Mean R²:   0.7702 ± 0.0049\n",
      "\n",
      " LGBM Classifier Performance Validation:\n",
      "Mean MSE:  42.0625 ± 1.1085\n",
      "Mean MAE:  5.1436 ± 0.1016\n",
      "Mean R²:   0.7250 ± 0.0013\n",
      "\n",
      " MLP Classifier Performance Validation:\n",
      "Mean MSE:  67.6820 ± 14.8293\n",
      "Mean MAE:  6.4986 ± 0.6182\n",
      "Mean R²:   0.5593 ± 0.0866\n",
      "\n",
      " TabPFN Classifier Performance Control:\n",
      "Mean MSE:  529.5271 ± 7.8860\n",
      "Mean MAE:  20.4306 ± 0.0899\n",
      "Mean R²:   -5.5850 ± 0.0981\n",
      "\n",
      " LGBM Classifier Performance Control:\n",
      "Mean MSE:  537.5037 ± 8.8568\n",
      "Mean MAE:  20.9807 ± 0.1776\n",
      "Mean R²:   -5.6842 ± 0.1101\n",
      "\n",
      " MLP Classifier Performance Control:\n",
      "Mean MSE:  414.0700 ± 16.4971\n",
      "Mean MAE:  18.4940 ± 0.5892\n",
      "Mean R²:   -4.1492 ± 0.2052\n",
      "\n",
      "=== Fold 4 ===\n",
      "Removing rh_transversetemporal_volume with p-value 0.9678\n",
      "Final Feature lengthe:  192\n",
      "Removing lh_parsorbitalis_volume with p-value 0.9572\n",
      "Final Feature lengthe:  191\n",
      "Removing lh_precuneus_thickness with p-value 0.9200\n",
      "Final Feature lengthe:  190\n",
      "Removing lh_rostralmiddlefrontal_thickness with p-value 0.8936\n",
      "Final Feature lengthe:  189\n",
      "Removing rh_precuneus_thickness with p-value 0.7841\n",
      "Final Feature lengthe:  188\n",
      "Removing Right-Amygdala with p-value 0.7390\n",
      "Final Feature lengthe:  187\n",
      "Removing rh_supramarginal_thickness with p-value 0.6456\n",
      "Final Feature lengthe:  186\n",
      "Removing rh_postcentral_thickness with p-value 0.6776\n",
      "Final Feature lengthe:  185\n",
      "Removing rh_cuneus_thickness with p-value 0.5838\n",
      "Final Feature lengthe:  184\n",
      "Removing lh_transversetemporal_thickness with p-value 0.5777\n",
      "Final Feature lengthe:  183\n",
      "Removing rh_bankssts_thickness with p-value 0.5729\n",
      "Final Feature lengthe:  182\n",
      "Removing rh_inferiortemporal_thickness with p-value 0.5335\n",
      "Final Feature lengthe:  181\n",
      "Removing lh_parahippocampal_thickness with p-value 0.4879\n",
      "Final Feature lengthe:  180\n",
      "Removing 5th-Ventricle with p-value 0.4703\n",
      "Final Feature lengthe:  179\n",
      "Removing rh_caudalanteriorcingulate_thickness with p-value 0.4654\n",
      "Final Feature lengthe:  178\n",
      "Removing lh_postcentral_thickness with p-value 0.4469\n",
      "Final Feature lengthe:  177\n",
      "Removing rh_entorhinal_thickness with p-value 0.4377\n",
      "Final Feature lengthe:  176\n",
      "Removing lh_parsopercularis_thickness with p-value 0.3608\n",
      "Final Feature lengthe:  175\n",
      "Removing lh_parsorbitalis_thickness with p-value 0.3371\n",
      "Final Feature lengthe:  174\n",
      "Removing rh_transversetemporal_thickness with p-value 0.3194\n",
      "Final Feature lengthe:  173\n",
      "Removing rh_rostralanteriorcingulate_thickness with p-value 0.2963\n",
      "Final Feature lengthe:  172\n",
      "Removing rh_medialorbitofrontal_thickness with p-value 0.3222\n",
      "Final Feature lengthe:  171\n",
      "Removing rh_lingual_thickness with p-value 0.2584\n",
      "Final Feature lengthe:  170\n",
      "Removing lh_posteriorcingulate_thickness with p-value 0.2444\n",
      "Final Feature lengthe:  169\n",
      "Removing rhCerebralWhiteMatterVol with p-value 0.2043\n",
      "Final Feature lengthe:  168\n",
      "Removing lhCerebralWhiteMatterVol with p-value 0.3750\n",
      "Final Feature lengthe:  167\n",
      "Removing rhCortexVol with p-value 0.2070\n",
      "Final Feature lengthe:  166\n",
      "Removing lhCortexVol with p-value 0.7482\n",
      "Final Feature lengthe:  165\n",
      "Removing lh_insula_thickness with p-value 0.2036\n",
      "Final Feature lengthe:  164\n",
      "Removing rh_lateralorbitofrontal_thickness with p-value 0.2244\n",
      "Final Feature lengthe:  163\n",
      "Removing rh_parsorbitalis_thickness with p-value 0.1860\n",
      "Final Feature lengthe:  162\n",
      "Removing rh_parsorbitalis_volume with p-value 0.2199\n",
      "Final Feature lengthe:  161\n",
      "Removing lh_parahippocampal_volume with p-value 0.2043\n",
      "Final Feature lengthe:  160\n",
      "Removing Left-Cerebellum-White-Matter with p-value 0.1228\n",
      "Final Feature lengthe:  159\n",
      "Removing BrainSegVol with p-value 0.4267\n",
      "Final Feature lengthe:  158\n",
      "Removing rh_cuneus_volume with p-value 0.1247\n",
      "Final Feature lengthe:  157\n",
      "Removing rh_frontalpole_volume with p-value 0.1620\n",
      "Final Feature lengthe:  156\n",
      "Removing lh_lateralorbitofrontal_volume with p-value 0.1423\n",
      "Final Feature lengthe:  155\n",
      "Removing CC_Central with p-value 0.1087\n",
      "Final Feature lengthe:  154\n",
      "Removing CerebralWhiteMatterVol with p-value 0.0910\n",
      "Final Feature lengthe:  153\n",
      "Removing Left-vessel with p-value 0.0861\n",
      "Final Feature lengthe:  152\n",
      "Removing non-WM-hypointensities with p-value 0.1756\n",
      "Final Feature lengthe:  151\n",
      "Removing lh_inferiortemporal_thickness with p-value 0.0847\n",
      "Final Feature lengthe:  150\n",
      "Removing lh_pericalcarine_thickness with p-value 0.0664\n",
      "Final Feature lengthe:  149\n",
      "Removing Right-Cerebellum-White-Matter with p-value 0.0600\n",
      "Final Feature lengthe:  148\n",
      "Removing lh_bankssts_thickness with p-value 0.0554\n",
      "Final Feature lengthe:  147\n",
      "Random Baseline Performance:\n",
      "MSE: 306.7589\n",
      "MAE: 14.0594\n",
      "R²:  -1.0114\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 56.0568\n",
      "MAE: 5.9322\n",
      "R²:  0.6324\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 458.3807\n",
      "MAE: 19.6183\n",
      "R²:  -4.7003\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 37.5621\n",
      "MAE: 4.7733\n",
      "R²:  0.7537\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 535.3640\n",
      "MAE: 20.4720\n",
      "R²:  -5.6576\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36210\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score 48.690250\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 45.2955\n",
      "MAE: 5.3635\n",
      "R²:  0.7030\n",
      "LightGBM Performance on Control:\n",
      "MSE: 541.9018\n",
      "MAE: 21.1573\n",
      "R²:  -5.7389\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      " Random Classifier Performance Validation:\n",
      "Mean MSE:  307.0253 ± 10.4833\n",
      "Mean MAE:  14.0975 ± 0.2124\n",
      "Mean R²:   -1.0086 ± 0.0347\n",
      "\n",
      " TabPFN Classifier Performance Validation:\n",
      "Mean MSE:  36.0177 ± 1.7495\n",
      "Mean MAE:  4.6967 ± 0.1128\n",
      "Mean R²:   0.7647 ± 0.0087\n",
      "\n",
      " LGBM Classifier Performance Validation:\n",
      "Mean MSE:  42.8708 ± 1.6974\n",
      "Mean MAE:  5.1985 ± 0.1297\n",
      "Mean R²:   0.7195 ± 0.0096\n",
      "\n",
      " MLP Classifier Performance Validation:\n",
      "Mean MSE:  64.7757 ± 13.7939\n",
      "Mean MAE:  6.3570 ± 0.5888\n",
      "Mean R²:   0.5776 ± 0.0814\n",
      "\n",
      " TabPFN Classifier Performance Control:\n",
      "Mean MSE:  531.4727 ± 7.0022\n",
      "Mean MAE:  20.4444 ± 0.0760\n",
      "Mean R²:   -5.6092 ± 0.0871\n",
      "\n",
      " LGBM Classifier Performance Control:\n",
      "Mean MSE:  538.6032 ± 7.9031\n",
      "Mean MAE:  21.0248 ± 0.1717\n",
      "Mean R²:   -5.6979 ± 0.0983\n",
      "\n",
      " MLP Classifier Performance Control:\n",
      "Mean MSE:  425.1476 ± 23.9220\n",
      "Mean MAE:  18.7751 ± 0.7053\n",
      "Mean R²:   -4.2870 ± 0.2975\n",
      "\n",
      "=== Fold 5 ===\n",
      "Removing Right-Amygdala with p-value 0.7907\n",
      "Final Feature lengthe:  192\n",
      "Removing rh_bankssts_thickness with p-value 0.7928\n",
      "Final Feature lengthe:  191\n",
      "Removing rh_caudalanteriorcingulate_thickness with p-value 0.7310\n",
      "Final Feature lengthe:  190\n",
      "Removing rh_supramarginal_thickness with p-value 0.7241\n",
      "Final Feature lengthe:  189\n",
      "Removing lh_parsorbitalis_thickness with p-value 0.7121\n",
      "Final Feature lengthe:  188\n",
      "Removing rh_medialorbitofrontal_thickness with p-value 0.7158\n",
      "Final Feature lengthe:  187\n",
      "Removing lh_rostralmiddlefrontal_thickness with p-value 0.6695\n",
      "Final Feature lengthe:  186\n",
      "Removing rh_precuneus_thickness with p-value 0.6246\n",
      "Final Feature lengthe:  185\n",
      "Removing rh_transversetemporal_volume with p-value 0.6133\n",
      "Final Feature lengthe:  184\n",
      "Removing lh_bankssts_thickness with p-value 0.5638\n",
      "Final Feature lengthe:  183\n",
      "Removing 5th-Ventricle with p-value 0.5484\n",
      "Final Feature lengthe:  182\n",
      "Removing rh_middletemporal_thickness with p-value 0.4992\n",
      "Final Feature lengthe:  181\n",
      "Removing rh_entorhinal_thickness with p-value 0.4900\n",
      "Final Feature lengthe:  180\n",
      "Removing rh_transversetemporal_thickness with p-value 0.4650\n",
      "Final Feature lengthe:  179\n",
      "Removing CerebralWhiteMatterVol with p-value 0.4477\n",
      "Final Feature lengthe:  178\n",
      "Removing rhCerebralWhiteMatterVol with p-value 0.7009\n",
      "Final Feature lengthe:  177\n",
      "Removing rhCortexVol with p-value 0.4486\n",
      "Final Feature lengthe:  176\n",
      "Removing lh_parahippocampal_thickness with p-value 0.4368\n",
      "Final Feature lengthe:  175\n",
      "Removing lh_parahippocampal_volume with p-value 0.4474\n",
      "Final Feature lengthe:  174\n",
      "Removing lh_lateralorbitofrontal_volume with p-value 0.6027\n",
      "Final Feature lengthe:  173\n",
      "Removing rh_rostralanteriorcingulate_thickness with p-value 0.4334\n",
      "Final Feature lengthe:  172\n",
      "Removing lh_caudalanteriorcingulate_volume with p-value 0.3640\n",
      "Final Feature lengthe:  171\n",
      "Removing lh_transversetemporal_thickness with p-value 0.3119\n",
      "Final Feature lengthe:  170\n",
      "Removing rh_pericalcarine_thickness with p-value 0.3128\n",
      "Final Feature lengthe:  169\n",
      "Removing lh_postcentral_thickness with p-value 0.3039\n",
      "Final Feature lengthe:  168\n",
      "Removing rh_postcentral_thickness with p-value 0.4153\n",
      "Final Feature lengthe:  167\n",
      "Removing rh_inferiortemporal_thickness with p-value 0.2823\n",
      "Final Feature lengthe:  166\n",
      "Removing lh_insula_thickness with p-value 0.2535\n",
      "Final Feature lengthe:  165\n",
      "Removing lh_posteriorcingulate_thickness with p-value 0.2058\n",
      "Final Feature lengthe:  164\n",
      "Removing non-WM-hypointensities with p-value 0.1722\n",
      "Final Feature lengthe:  163\n",
      "Removing Left-vessel with p-value 0.2812\n",
      "Final Feature lengthe:  162\n",
      "Removing rh_parsorbitalis_thickness with p-value 0.1485\n",
      "Final Feature lengthe:  161\n",
      "Removing Left-non-WM-hypointensities with p-value 0.6708\n",
      "Final Feature lengthe:  160\n",
      "Removing rh_lateralorbitofrontal_thickness with p-value 0.1565\n",
      "Final Feature lengthe:  159\n",
      "Removing lh_inferiortemporal_thickness with p-value 0.1377\n",
      "Final Feature lengthe:  158\n",
      "Removing lh_parstriangularis_volume with p-value 0.1244\n",
      "Final Feature lengthe:  157\n",
      "Removing CC_Mid_Anterior with p-value 0.1145\n",
      "Final Feature lengthe:  156\n",
      "Removing rh_lingual_thickness with p-value 0.1008\n",
      "Final Feature lengthe:  155\n",
      "Removing rh_precentral_thickness with p-value 0.1022\n",
      "Final Feature lengthe:  154\n",
      "Removing rh_cuneus_thickness with p-value 0.0699\n",
      "Final Feature lengthe:  153\n",
      "Random Baseline Performance:\n",
      "MSE: 315.4013\n",
      "MAE: 14.2763\n",
      "R²:  -1.0288\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "MLP Performance on Validation:\n",
      "MSE: 58.7519\n",
      "MAE: 6.0510\n",
      "R²:  0.6221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MLP Performance on Control:\n",
      "MSE: 551.7192\n",
      "MAE: 21.8817\n",
      "R²:  -5.8610\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      "TabPFN Regressor Performance on Validation:\n",
      "MSE: 35.0396\n",
      "MAE: 4.6421\n",
      "R²:  0.7746\n",
      "TabPFN Regressor Performance on Control:\n",
      "MSE: 531.5572\n",
      "MAE: 20.5169\n",
      "R²:  -5.6103\n",
      "CUDA memory cleared and model deleted.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37995\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 149\n",
      "[LightGBM] [Info] Start training from score 48.690375\n",
      "\n",
      "LightGBM Performance on Validation:\n",
      "MSE: 42.2419\n",
      "MAE: 5.1553\n",
      "R²:  0.7283\n",
      "LightGBM Performance on Control:\n",
      "MSE: 553.7058\n",
      "MAE: 21.2603\n",
      "R²:  -5.8857\n",
      "CUDA memory cleared and model deleted.\n",
      "\n",
      " Random Classifier Performance Validation:\n",
      "Mean MSE:  308.7005 ± 9.9572\n",
      "Mean MAE:  14.1333 ± 0.2030\n",
      "Mean R²:   -1.0126 ± 0.0321\n",
      "\n",
      " TabPFN Classifier Performance Validation:\n",
      "Mean MSE:  35.7732 ± 1.5732\n",
      "Mean MAE:  4.6831 ± 0.1005\n",
      "Mean R²:   0.7672 ± 0.0087\n",
      "\n",
      " LGBM Classifier Performance Validation:\n",
      "Mean MSE:  42.7450 ± 1.5389\n",
      "Mean MAE:  5.1899 ± 0.1173\n",
      "Mean R²:   0.7212 ± 0.0093\n",
      "\n",
      " MLP Classifier Performance Validation:\n",
      "Mean MSE:  63.5709 ± 12.5707\n",
      "Mean MAE:  6.2958 ± 0.5407\n",
      "Mean R²:   0.5865 ± 0.0750\n",
      "\n",
      " TabPFN Classifier Performance Control:\n",
      "Mean MSE:  531.4938 ± 6.0642\n",
      "Mean MAE:  20.4625 ± 0.0729\n",
      "Mean R²:   -5.6095 ± 0.0754\n",
      "\n",
      " LGBM Classifier Performance Control:\n",
      "Mean MSE:  541.6237 ± 9.2985\n",
      "Mean MAE:  21.0719 ± 0.1802\n",
      "Mean R²:   -5.7354 ± 0.1156\n",
      "\n",
      " MLP Classifier Performance Control:\n",
      "Mean MSE:  450.4619 ± 54.9642\n",
      "Mean MAE:  19.3964 ± 1.3936\n",
      "Mean R²:   -4.6018 ± 0.6835\n",
      "\n",
      "=== Deconfounding Strategy: Correlation ===\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     X_train_scaled, X_val_scaled, X_control_scaled\u001b[38;5;241m=\u001b[39m feature_extration_with_PCA(X_train_scaled, X_val_scaled, X_control_scaled,  n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m deconfounding_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 44\u001b[0m     X_train_scaled, X_val_scaled, X_control_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction_with_Pearson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_control_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Random Baseline\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# We'll generate random predictions from a normal distribution \u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# matching the train target's mean and std\u001b[39;00m\n\u001b[1;32m     51\u001b[0m random_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mmean(), scale\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mstd(), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_val))\n",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m, in \u001b[0;36mfeature_extraction_with_Pearson\u001b[0;34m(X, X_val, X_control, y, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_extraction_with_Pearson\u001b[39m(X, X_val, X_control, y, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     correlation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m()\u001b[38;5;241m.\u001b[39mabs()\n\u001b[1;32m      3\u001b[0m     upper \u001b[38;5;241m=\u001b[39m correlation_matrix\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones(correlation_matrix\u001b[38;5;241m.\u001b[39mshape), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m))\n\u001b[1;32m      4\u001b[0m     to_drop \u001b[38;5;241m=\u001b[39m [column \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m upper\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(upper[column] \u001b[38;5;241m>\u001b[39m threshold)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'corr'"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# K-Fold Cross-Validation Setup\n",
    "###############################################################################\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "###############################################################################\n",
    "# Training & Evaluation\n",
    "###############################################################################\n",
    "mlp_results = []\n",
    "lgb_results = []\n",
    "tabpfn_results = []\n",
    "random_results = []\n",
    "\n",
    "mlp_results_eval = []\n",
    "lgb_results_eval = []\n",
    "tabpfn_results_eval = []\n",
    "result_dict = {}\n",
    "\n",
    "model_dict = {}\n",
    "best_mse_mlp = float('inf')\n",
    "best_mse_lgb = float('inf')\n",
    "best_mse_tab = float('inf')\n",
    "deconfounding_strategies = [\"BE\", \"Correlation\", \"PCA\" \"Nothing\"]\n",
    "for deconfounding_strategy in deconfounding_strategies:\n",
    "    print(f\"\\n=== Deconfounding Strategy: {deconfounding_strategy} ===\")\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Scale features\n",
    "        df_columns = X.columns\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled   = scaler.transform(X_val)\n",
    "        X_control_scaled = scaler.fit_transform(X_control)\n",
    "\n",
    "        if deconfounding_strategy == \"BE\":\n",
    "            X_train_scaled, X_val_scaled, X_control_scaled = feature_extration_with_BE(X_train_scaled, X_val_scaled, X_control_scaled, y_train, df_columns=df_columns)\n",
    "        elif deconfounding_strategy == \"PCA\":\n",
    "            X_train_scaled, X_val_scaled, X_control_scaled= feature_extration_with_PCA(X_train_scaled, X_val_scaled, X_control_scaled,  n_components=50)\n",
    "        elif deconfounding_strategy == \"Correlation\":\n",
    "            X_train_scaled, X_val_scaled, X_control_scaled = feature_extraction_with_Pearson(X_train_scaled, X_val_scaled, X_control_scaled, y_train, threshold=0.6, df_columns=df_columns)\n",
    "\n",
    "        # -------------------------------\n",
    "        # Random Baseline\n",
    "        # -------------------------------\n",
    "        # We'll generate random predictions from a normal distribution \n",
    "        # matching the train target's mean and std\n",
    "        random_predictions = np.random.normal(loc=y_train.mean(), scale=y_train.std(), size=len(y_val))\n",
    "        random_perf = evaluate_regression_performance(y_val, random_predictions)\n",
    "        print(\"Random Baseline Performance:\")\n",
    "        print_regression_performance(random_perf)\n",
    "        random_results.append(random_perf)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # MLP\n",
    "        # -------------------------------\n",
    "        # KerasRegressor or direct model\n",
    "        mlp_model = create_mlp_model(input_shape=X_train_scaled.shape[1])\n",
    "        mlp_model.fit(X_train_scaled, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=32,\n",
    "                    verbose=0)\n",
    "        \n",
    "        y_pred_mlp = mlp_model.predict(X_val_scaled).ravel()  # ensure shape (n,)\n",
    "        mlp_perf = evaluate_regression_performance(y_val, y_pred_mlp)\n",
    "        print(\"\\nMLP Performance on Validation:\")\n",
    "        print_regression_performance(mlp_perf)\n",
    "        mlp_results.append(mlp_perf)\n",
    "        \n",
    "        # Evaluate on control data\n",
    "        y_pred_mlp_ctrl = mlp_model.predict(X_control_scaled).ravel()\n",
    "        mlp_perf_ctrl = evaluate_regression_performance(y_control, y_pred_mlp_ctrl)\n",
    "        print(\"MLP Performance on Control:\")\n",
    "        print_regression_performance(mlp_perf_ctrl)\n",
    "        mlp_results_eval.append(mlp_perf_ctrl)\n",
    "        \n",
    "        # Keep best MLP model based on MSE\n",
    "        if mlp_perf['mse'] < best_mse_mlp:\n",
    "            best_mse_mlp = mlp_perf['mse']\n",
    "            model_dict[\"mlp\"] = mlp_model\n",
    "        \n",
    "        # Clean up\n",
    "        clean_up_cuda(mlp_model)\n",
    "\n",
    "        # -------------------------------\n",
    "        # (Hypothetical) TabPFN Regressor\n",
    "        # -------------------------------\n",
    "        # NOTE: If TabPFNClassifier is the only option, you must skip or replace this.\n",
    "        try:\n",
    "            tabclf = TabPFNRegressor()  # Ideally TabPFNRegressor() if available\n",
    "            tabclf.fit(X_train_scaled, y_train)\n",
    "            y_pred_tab = tabclf.predict(X_val_scaled)  # For regression, this should be continuous\n",
    "            tab_perf = evaluate_regression_performance(y_val, y_pred_tab)\n",
    "            print(\"\\nTabPFN Regressor Performance on Validation:\")\n",
    "            print_regression_performance(tab_perf)\n",
    "            tabpfn_results.append(tab_perf)\n",
    "            \n",
    "            # Evaluate on control data\n",
    "            y_pred_tab_ctrl = tabclf.predict(X_control_scaled)\n",
    "            tab_perf_ctrl = evaluate_regression_performance(y_control, y_pred_tab_ctrl)\n",
    "            print(\"TabPFN Regressor Performance on Control:\")\n",
    "            print_regression_performance(tab_perf_ctrl)\n",
    "            tabpfn_results_eval.append(tab_perf_ctrl)\n",
    "            \n",
    "            if tab_perf['mse'] < best_mse_tab:\n",
    "                best_mse_tab = tab_perf['mse']\n",
    "                model_dict[\"tabpfn\"] = tabclf\n",
    "            \n",
    "            clean_up_cuda(tabclf)\n",
    "        except Exception as e:\n",
    "            print(\"TabPFN Regressor not available or failed. Skipping...\")\n",
    "            print(e)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # LightGBM\n",
    "        # -------------------------------\n",
    "        lgb_train = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "        lgb_eval  = lgb.Dataset(X_val_scaled,   label=y_val, reference=lgb_train)\n",
    "        \n",
    "        lgb_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        lgbclf = lgb.train(\n",
    "            params=lgb_params, \n",
    "            train_set=lgb_train, \n",
    "            valid_sets=[lgb_train, lgb_eval], \n",
    "            num_boost_round=1000\n",
    "        )\n",
    "        \n",
    "        y_pred_lgb = lgbclf.predict(X_val_scaled)\n",
    "        lgb_perf = evaluate_regression_performance(y_val, y_pred_lgb)\n",
    "        print(\"\\nLightGBM Performance on Validation:\")\n",
    "        print_regression_performance(lgb_perf)\n",
    "        lgb_results.append(lgb_perf)\n",
    "        \n",
    "        # Evaluate on control data\n",
    "        y_pred_lgb_ctrl = lgbclf.predict(X_control_scaled)\n",
    "        lgb_perf_ctrl = evaluate_regression_performance(y_control, y_pred_lgb_ctrl)\n",
    "        print(\"LightGBM Performance on Control:\")\n",
    "        print_regression_performance(lgb_perf_ctrl)\n",
    "        lgb_results_eval.append(lgb_perf_ctrl)\n",
    "        \n",
    "        if lgb_perf['mse'] < best_mse_lgb:\n",
    "            best_mse_lgb = lgb_perf['mse']\n",
    "            model_dict[\"lgb\"] = lgbclf\n",
    "        \n",
    "        clean_up_cuda(lgbclf)\n",
    "\n",
    "        random_summary = aggregate_cv_metrics_and_print(random_results, \"Random\")\n",
    "        tabpfn_summary = aggregate_cv_metrics_and_print(tabpfn_results, \"TabPFN\")\n",
    "        lgb_summary = aggregate_cv_metrics_and_print(lgb_results, \"LGBM\")\n",
    "        mlp_summary = aggregate_cv_metrics_and_print(mlp_results, \"MLP\")\n",
    "\n",
    "        tabpfn_eval_summary = aggregate_cv_metrics_and_print(tabpfn_results_eval, \"TabPFN\", \"Control\")\n",
    "        lgb_eval_summary = aggregate_cv_metrics_and_print(lgb_results_eval, \"LGBM\", \"Control\")\n",
    "        mlp_eval_summary = aggregate_cv_metrics_and_print(mlp_results_eval, \"MLP\", \"Control\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result_dict[deconfounding_strategy] = {\n",
    "        \"TabPFN\": {\n",
    "                \"results\": tabpfn_summary,\n",
    "                \"results_eval\": tabpfn_eval_summary,\n",
    "                \"cv_results\": tabpfn_results,\n",
    "                \"cv_results_eval\": tabpfn_results_eval\n",
    "        },\n",
    "        \"LGBM\": {\n",
    "                \"results\": lgb_summary,\n",
    "                \"results_eval\": lgb_eval_summary,\n",
    "                \"cv_results\": lgb_results,\n",
    "                \"cv_results_eval\": lgb_results_eval\n",
    "        },\n",
    "        \"Random\": {\n",
    "                \"results\": random_summary,\n",
    "                \"cv_results\": random_results\n",
    "        },\n",
    "        \"MLP\": {\n",
    "                \"results\": mlp_summary,\n",
    "                \"results_eval\": mlp_eval_summary,\n",
    "                \"cv_results\": mlp_results,\n",
    "                \"cv_results_eval\": mlp_results_eval\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BE': {'TabPFN': {'results': {'mean_mse': 35.77315711975098,\n",
       "    'std_mse': 1.573190204542749,\n",
       "    'mean_mae': 4.68308687210083,\n",
       "    'std_mae': 0.10049323750711442,\n",
       "    'mean_r2': 0.7671690285205841,\n",
       "    'std_r2': 0.008691661191930461},\n",
       "   'results_eval': {'mean_mse': 531.4938320098129,\n",
       "    'std_mse': 6.0641905975681825,\n",
       "    'mean_mae': 20.462524838028592,\n",
       "    'std_mae': 0.0729228018659534,\n",
       "    'mean_r2': -5.609467662353669,\n",
       "    'std_r2': 0.07541211061925573},\n",
       "   'cv_results': [{'mse': 36.919368743896484,\n",
       "     'mae': 4.779675483703613,\n",
       "     'r2': 0.765300452709198},\n",
       "    {'mse': 33.571475982666016,\n",
       "     'mae': 4.53729772567749,\n",
       "     'r2': 0.7750607132911682},\n",
       "    {'mse': 37.56214141845703,\n",
       "     'mae': 4.773266792297363,\n",
       "     'r2': 0.7537106871604919},\n",
       "    {'mse': 35.039642333984375,\n",
       "     'mae': 4.6421074867248535,\n",
       "     'r2': 0.7746042609214783}],\n",
       "   'cv_results_eval': [{'mse': 537.4131055020864,\n",
       "     'mae': 20.52050816763395,\n",
       "     'r2': -5.683077635556684},\n",
       "    {'mse': 521.6410296175645,\n",
       "     'mae': 20.340628169071724,\n",
       "     'r2': -5.486941727200533},\n",
       "    {'mse': 535.3639871125621,\n",
       "     'mae': 20.472035815027468,\n",
       "     'r2': -5.657595530372732},\n",
       "    {'mse': 531.5572058070385,\n",
       "     'mae': 20.51692720038123,\n",
       "     'r2': -5.6102557562847295}]},\n",
       "  'LGBM': {'results': {'mean_mse': 42.74500688696226,\n",
       "    'std_mse': 1.5389329153562872,\n",
       "    'mean_mae': 5.189881321734409,\n",
       "    'std_mae': 0.11727461172112856,\n",
       "    'mean_r2': 0.7212363869498406,\n",
       "    'std_r2': 0.00925713140078002},\n",
       "   'results_eval': {'mean_mse': 541.6237377965875,\n",
       "    'std_mse': 9.298462733861317,\n",
       "    'mean_mae': 21.071927055111864,\n",
       "    'std_mae': 0.18017102261566795,\n",
       "    'mean_r2': -5.735439556452978,\n",
       "    'std_r2': 0.1156323649451541},\n",
       "   'cv_results': [{'mse': 43.51836799986873,\n",
       "     'mae': 5.278465076093474,\n",
       "     'r2': 0.7233500734861842},\n",
       "    {'mse': 41.83805925790044,\n",
       "     'mae': 5.119071973479843,\n",
       "     'r2': 0.7251343334705223},\n",
       "    {'mse': 40.83116410730914,\n",
       "     'mae': 5.033118013903631,\n",
       "     'r2': 0.7264185865517128},\n",
       "    {'mse': 45.29550796903475,\n",
       "     'mae': 5.363471726773342,\n",
       "     'r2': 0.7030041112370349},\n",
       "    {'mse': 42.24193510069822,\n",
       "     'mae': 5.155279818421752,\n",
       "     'r2': 0.7282748300037492}],\n",
       "   'cv_results_eval': [{'mse': 548.2106007507174,\n",
       "     'mae': 21.229849668938066,\n",
       "     'r2': -5.817351434013343},\n",
       "    {'mse': 537.7793487313764,\n",
       "     'mae': 20.883400730901368,\n",
       "     'r2': -5.687632105683636},\n",
       "    {'mse': 526.521170684297,\n",
       "     'mae': 20.828870348702022,\n",
       "     'r2': -5.54762942031321},\n",
       "    {'mse': 541.9017733282525,\n",
       "     'mae': 21.1572563621426,\n",
       "     'r2': -5.73889710712403},\n",
       "    {'mse': 553.7057954882941,\n",
       "     'mae': 21.260258164875257,\n",
       "     'r2': -5.885687715130674}]},\n",
       "  'Random': {'results': {'mean_mse': 308.70049871366416,\n",
       "    'std_mse': 9.957163938008048,\n",
       "    'mean_mae': 14.13329088373855,\n",
       "    'std_mae': 0.2029932435825036,\n",
       "    'mean_r2': -1.0126136228046643,\n",
       "    'std_r2': 0.03211971467955283},\n",
       "   'cv_results': [{'mse': 320.48866400206583,\n",
       "     'mae': 14.314748545863704,\n",
       "     'r2': -1.0373733993184207},\n",
       "    {'mse': 309.6785058093978,\n",
       "     'mae': 14.248556435275644,\n",
       "     'r2': -1.0345109314094048},\n",
       "    {'mse': 291.175147064141,\n",
       "     'mae': 13.767367511923824,\n",
       "     'r2': -0.9509634377669067},\n",
       "    {'mse': 306.75889721332743,\n",
       "     'mae': 14.059438186913875,\n",
       "     'r2': -1.0113723280485534},\n",
       "    {'mse': 315.4012794793886,\n",
       "     'mae': 14.276343738715708,\n",
       "     'r2': -1.0288480174800352}]},\n",
       "  'MLP': {'results': {'mean_mse': 63.570925903320315,\n",
       "    'std_mse': 12.570704386041438,\n",
       "    'mean_mae': 6.295804691314697,\n",
       "    'std_mae': 0.5407219148138008,\n",
       "    'mean_r2': 0.5864742755889892,\n",
       "    'std_r2': 0.07498681988432376},\n",
       "   'results_eval': {'mean_mse': 450.46194609753564,\n",
       "    'std_mse': 54.96421301161385,\n",
       "    'mean_mae': 19.396429522366702,\n",
       "    'std_mae': 1.3935610564089338,\n",
       "    'mean_r2': -4.601784040642627,\n",
       "    'std_r2': 0.6835153422444109},\n",
       "   'cv_results': [{'mse': 87.82698059082031,\n",
       "     'mae': 7.305653095245361,\n",
       "     'r2': 0.44167643785476685},\n",
       "    {'mse': 62.65891647338867,\n",
       "     'mae': 6.386112213134766,\n",
       "     'r2': 0.5883464813232422},\n",
       "    {'mse': 52.56002426147461,\n",
       "     'mae': 5.8040361404418945,\n",
       "     'r2': 0.6478315591812134},\n",
       "    {'mse': 56.05677795410156,\n",
       "     'mae': 5.932248592376709,\n",
       "     'r2': 0.6324441432952881},\n",
       "    {'mse': 58.751930236816406,\n",
       "     'mae': 6.050973415374756,\n",
       "     'r2': 0.6220727562904358}],\n",
       "   'cv_results_eval': [{'mse': 396.78378464775994,\n",
       "     'mae': 18.247590491961237,\n",
       "     'r2': -3.9342615767688764},\n",
       "    {'mse': 409.14391704855177,\n",
       "     'mae': 17.92790337486746,\n",
       "     'r2': -4.087967773314042},\n",
       "    {'mse': 436.2821852267282,\n",
       "     'mae': 19.3066556723048,\n",
       "     'r2': -4.425449593672452},\n",
       "    {'mse': 458.3806641187825,\n",
       "     'mae': 19.618340580034456,\n",
       "     'r2': -4.700258392622994},\n",
       "    {'mse': 551.7191794458557,\n",
       "     'mae': 21.881657492665568,\n",
       "     'r2': -5.860982866834773}]}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deconfounding Strategy: BE ===\n",
      "  TabPFN - Results: {'mean_mse': 35.77315711975098, 'std_mse': 1.573190204542749, 'mean_mae': 4.68308687210083, 'std_mae': 0.10049323750711442, 'mean_r2': 0.7671690285205841, 'std_r2': 0.008691661191930461}\n",
      "  TabPFN - Evaluation Results: {'mean_mse': 531.4938320098129, 'std_mse': 6.0641905975681825, 'mean_mae': 20.462524838028592, 'std_mae': 0.0729228018659534, 'mean_r2': -5.609467662353669, 'std_r2': 0.07541211061925573}\n",
      "  LGBM - Results: {'mean_mse': 42.74500688696226, 'std_mse': 1.5389329153562872, 'mean_mae': 5.189881321734409, 'std_mae': 0.11727461172112856, 'mean_r2': 0.7212363869498406, 'std_r2': 0.00925713140078002}\n",
      "  LGBM - Evaluation Results: {'mean_mse': 541.6237377965875, 'std_mse': 9.298462733861317, 'mean_mae': 21.071927055111864, 'std_mae': 0.18017102261566795, 'mean_r2': -5.735439556452978, 'std_r2': 0.1156323649451541}\n",
      "  Random - Results: {'mean_mse': 308.70049871366416, 'std_mse': 9.957163938008048, 'mean_mae': 14.13329088373855, 'std_mae': 0.2029932435825036, 'mean_r2': -1.0126136228046643, 'std_r2': 0.03211971467955283}\n",
      "  MLP - Results: {'mean_mse': 63.570925903320315, 'std_mse': 12.570704386041438, 'mean_mae': 6.295804691314697, 'std_mae': 0.5407219148138008, 'mean_r2': 0.5864742755889892, 'std_r2': 0.07498681988432376}\n",
      "  MLP - Evaluation Results: {'mean_mse': 450.46194609753564, 'std_mse': 54.96421301161385, 'mean_mae': 19.396429522366702, 'std_mae': 1.3935610564089338, 'mean_r2': -4.601784040642627, 'std_r2': 0.6835153422444109}\n"
     ]
    }
   ],
   "source": [
    "for result, models in result_dict.items():\n",
    "    print(f\"\\n=== Deconfounding Strategy: {result} ===\")\n",
    "    for model, results in models.items():\n",
    "        print(f\"  {model} - Results: {results['results']}\")\n",
    "        if 'results_eval' in results:\n",
    "            print(f\"  {model} - Evaluation Results: {results['results_eval']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pickle\\nsave_dir = \"../98_models/\"\\nwith open(os.path.join(save_dir, \"best_regressor.pkl\"), \"rb\") as f:\\n    loaded_model = pickle.load(f)\\n    # For example, if it\\'s a LightGBM model, you can just do:\\n    y_pred_control = loaded_model.predict(X_control_scaled)\\n    performance_control = evaluate_regression_performance(y_control, y_pred_control)\\n    print(\"\\nLoaded Model Performance on Control Data:\")\\n    print_regression_performance(performance_control)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Example: Load a saved model & evaluate on control data\n",
    "###############################################################################\n",
    "# If you have a saved regression model:\n",
    "\"\"\"\n",
    "import pickle\n",
    "save_dir = \"../98_models/\"\n",
    "with open(os.path.join(save_dir, \"best_regressor.pkl\"), \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "    # For example, if it's a LightGBM model, you can just do:\n",
    "    y_pred_control = loaded_model.predict(X_control_scaled)\n",
    "    performance_control = evaluate_regression_performance(y_control, y_pred_control)\n",
    "    print(\"\\nLoaded Model Performance on Control Data:\")\n",
    "    print_regression_performance(performance_control)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NAKO_CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
