{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_metric_by_feature_extraction(df, models, metric='AUC', show_eval=True, show_train=True):\n",
    "    df['Feature_extraction_method'] = df['search_term'].apply(lambda x: x.split('_', 1)[1])\n",
    "    df[\"Feature_extraction_method\"] = df[\"Feature_extraction_method\"].apply(\n",
    "    lambda x: \"_\".join(x.split(\"_\")[:-2]))\n",
    "    # Display unique values to verify the result\n",
    "    df[\"Feature_extraction_method\"].unique()\n",
    "    best_model_train_score = 0\n",
    "    best_model_eval_score = 0\n",
    "    best_train_search_term = \"\"\n",
    "    best_eval_search_term = \"\"\n",
    "    feature_extraction_methods = df['Feature_extraction_method'].unique()\n",
    "    \n",
    "    # Define a color palette for models\n",
    "    color_palette = sns.color_palette('tab10', len(models))\n",
    "    model_colors = dict(zip(models, color_palette))\n",
    "\n",
    "    # Generate plots for each feature extraction method with matching colors for solid and dotted lines\n",
    "    for method in feature_extraction_methods:\n",
    "        \n",
    "        # Filter data for the current feature extraction method\n",
    "        method_df = df[df['Feature_extraction_method'] == method]\n",
    "        \n",
    "        # Group by model type, percentage of data, and eval/train, then calculate mean metrics\n",
    "        grouped_method_df = method_df.groupby(\n",
    "            [\"percentage_of_data\", \"model_type\", \"eval_or_train\", \"search_term\"]\n",
    "        )[[metric]].mean().reset_index()\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        grouped_method_df_train = grouped_method_df[grouped_method_df[\"eval_or_train\"] == \"train\"]\n",
    "        grouped_method_df_eval = grouped_method_df[grouped_method_df[\"eval_or_train\"] == \"eval\"]\n",
    "\n",
    "        # Plot lines for each model type\n",
    "        for model in models:\n",
    "            if model == \"Random\":\n",
    "                continue\n",
    "\n",
    "            train_data = grouped_method_df_train[grouped_method_df_train['model_type'] == model]\n",
    "            eval_data = grouped_method_df_eval[grouped_method_df_eval['model_type'] == model]\n",
    "\n",
    "            # Use the same color for both train and eval lines\n",
    "            color = model_colors[model]\n",
    "\n",
    "            # Plot train data (solid line)\n",
    "            if show_train:\n",
    "                plt.plot(train_data['percentage_of_data'], train_data[metric], label=f'{model} (train)', linestyle='-', color=color)\n",
    "\n",
    "            # Plot eval data (dotted line) if enabled\n",
    "            if show_eval:\n",
    "                plt.plot(eval_data['percentage_of_data'], eval_data[metric], label=f'{model} (eval)', linestyle='--', color=color)\n",
    "            \n",
    "            # Update best model scores\n",
    "            if train_data[metric].max() > best_model_train_score:\n",
    "                best_model_train_score = train_data[metric].max()\n",
    "                best_train_search_term = train_data[train_data[metric] == best_model_train_score].iloc[0]['search_term']\n",
    "            if eval_data[metric].max() > best_model_eval_score:\n",
    "                best_model_eval_score = eval_data[metric].max()\n",
    "                best_eval_search_term = eval_data[eval_data[metric] == best_model_eval_score].iloc[0]['search_term']\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.xlabel('Percentage of Data')\n",
    "        plt.ylabel(f'Mean {metric.replace(\"_\", \" \")}')\n",
    "        plt.title(f'{metric.replace(\"_\", \" \")} vs Percentage of Data by Model - Feature Extraction: {method}')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Best model train score: {best_model_train_score} with search term: {best_train_search_term}\")\n",
    "    print(f\"Best model eval score: {best_model_eval_score} with search term: {best_eval_search_term}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"00_results/wm/\"\n",
    "csvs_deconfounded = [path + f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "df = pd.concat([pd.read_csv(file) for file in csvs_deconfounded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df['model_type'].unique()\n",
    "plot_metric_by_feature_extraction(df, models, metric='AUC', show_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_feature_extraction(df, models, metric='Balanced_ACC', show_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./00_results/age_unausgeglichen_10classes/viele_perc/\"\n",
    "csvs_deconfounded = [path + f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "df = pd.concat([pd.read_csv(file) for file in csvs_deconfounded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df['model_type'].unique()\n",
    "plot_metric_by_feature_extraction(df, models, metric='AUC', show_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_feature_extraction(df, models, metric='Balanced_ACC', show_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2_gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
