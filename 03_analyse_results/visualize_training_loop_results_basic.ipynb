{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from IPython.display import display\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for analysing:\n",
    "\n",
    "\n",
    "def get_statistics_summary(df, columns_to_analyze, group_by_col=\"label_col\", \n",
    "                         metrics=[\"mean\", \"std\", \"max\"], sort_by=None, ascending=False):\n",
    "    # Calculate statistics for all specified columns\n",
    "    summary = df.groupby(group_by_col).agg({\n",
    "        col: metrics for col in columns_to_analyze\n",
    "    })\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary.columns = [f\"{col[0]}_{col[1]}\" for col in summary.columns]\n",
    "    \n",
    "    # Sort if specified\n",
    "    if sort_by is not None:\n",
    "        summary = summary.sort_values(sort_by, ascending=ascending)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def count_partial_combinations(df, min_features=2, max_features=None):\n",
    "    # Set max_features to the length of the largest combination if not specified\n",
    "    if max_features is None:\n",
    "        max_features = max(len(features) for features in df[\"features\"])\n",
    "    \n",
    "    # Counter for partial combinations\n",
    "    combination_counts = Counter()\n",
    "\n",
    "    # Iterate over all rows in the DataFrame\n",
    "    for feature_list in df[\"features\"]:\n",
    "        # Generate all possible subsets (partial combinations)\n",
    "        for r in range(min_features, max_features + 1):\n",
    "            for subset in combinations(sorted(feature_list), r):  # Sort to ensure order invariance\n",
    "                combination_counts[subset] += 1\n",
    "\n",
    "    # Convert the Counter to a DataFrame for better visualization\n",
    "    results_df = pd.DataFrame.from_dict(combination_counts, orient=\"index\", columns=[\"count\"])\n",
    "    results_df.index = results_df.index.map(lambda x: \"+\".join(x))  # Convert tuples to strings\n",
    "    results_df = results_df.sort_values(\"count\", ascending=False)\n",
    "    \n",
    "    return results_df\n",
    "def pretraining_featureextraction_analysis(df):\n",
    "    #Pretraining Analyses\n",
    "    pretrained_summary = df.groupby(\"Pretraining_applied\")[\"Balanced_ACC\"].mean()\n",
    "    display(pretrained_summary)\n",
    "\n",
    "    pretrained_data = df[df[\"Pretraining_applied\"]==True][\"Balanced_ACC\"]\n",
    "    non_pretrained_data = df[df[\"Pretraining_applied\"]==False][\"Balanced_ACC\"]\n",
    "    t_stat, p_val = ttest_ind(pretrained_data, non_pretrained_data)\n",
    "    print(\"T-test Pretrained vs Non-pretrained:\", t_stat, p_val)\n",
    "\n",
    "    #Feature extraction analyses\n",
    "    feature_extraction_summary = df.groupby(\"Feature_extraction_applied\")[\"Balanced_ACC\"].mean()\n",
    "    display(feature_extraction_summary)\n",
    "\n",
    "    feature_extraction_data = df[df[\"Feature_extraction_applied\"]==True][\"Balanced_ACC\"]\n",
    "    non_feature_extraction_data = df[df[\"Feature_extraction_applied\"]==False][\"Balanced_ACC\"]\n",
    "    t_stat, p_val = ttest_ind(feature_extraction_data, non_feature_extraction_data)\n",
    "    print(\"T-test Feature extraction vs Non-feature extraction:\", t_stat, p_val)\n",
    "\n",
    "def feature_statistical_testing_easy(df, metric_column=\"mean\"):\n",
    "    # Get a unique list of features\n",
    "    all_features = set(f for feature_list in df[\"features\"] for f in feature_list)\n",
    "    for feature in all_features:\n",
    "        df[feature] = df[\"features\"].apply(lambda x: 1 if feature in x else 0)\n",
    "    \n",
    "    # Perform statistical testing\n",
    "    results = {}\n",
    "    for feature in all_features:\n",
    "        group_with_feature = df[df[feature] == 1][metric_column]\n",
    "        group_without_feature = df[df[feature] == 0][metric_column]\n",
    "        t_stat, p_value = ttest_ind(group_with_feature, group_without_feature, equal_var=False)\n",
    "        results[feature] = {\"t-statistic\": t_stat, \"p-value\": p_value}\n",
    "    \n",
    "    # Convert results to a DataFrame for better visualization\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df = results_df.sort_values(\"p-value\")\n",
    "    display(results_df)\n",
    "    return results_df\n",
    "\n",
    "def feature_statistical_testing(df, metric_column=\"mean\", alpha=0.05, correction_method='fdr_bh'):\n",
    "    \"\"\"\n",
    "    Perform statistical testing for feature significance using independent t-tests\n",
    "    with multiple testing correction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing a 'features' column with lists of features and a metric column\n",
    "    metric_column : str, default=\"mean\"\n",
    "        Name of the column containing the metric to test\n",
    "    alpha : float, default=0.05\n",
    "        Significance level for hypothesis testing\n",
    "    correction_method : str, default='fdr_bh'\n",
    "        Multiple testing correction method ('fdr_bh', 'bonferroni', etc.)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Results containing t-statistics, p-values, and adjusted p-values\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if \"features\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'features' column\")\n",
    "    if metric_column not in df.columns:\n",
    "        raise ValueError(f\"DataFrame must contain the metric column: {metric_column}\")\n",
    "    \n",
    "    # Get unique features\n",
    "    all_features = set(f for feature_list in df[\"features\"] for f in feature_list)\n",
    "    \n",
    "    # Create binary columns for each feature\n",
    "    feature_columns = {}\n",
    "    for feature in all_features:\n",
    "        feature_columns[feature] = df[\"features\"].apply(lambda x: 1 if feature in x else 0)\n",
    "    \n",
    "    # Add binary columns to DataFrame\n",
    "    feature_df = pd.DataFrame(feature_columns)\n",
    "    df = pd.concat([df, feature_df], axis=1)\n",
    "    \n",
    "    # Perform statistical testing\n",
    "    results = {}\n",
    "    p_values = []\n",
    "    \n",
    "    for feature in all_features:\n",
    "        # Get groups\n",
    "        group_with_feature = df[df[feature] == 1][metric_column]\n",
    "        group_without_feature = df[df[feature] == 0][metric_column]\n",
    "        \n",
    "        # Skip if either group is empty\n",
    "        if len(group_with_feature) == 0 or len(group_without_feature) == 0:\n",
    "            results[feature] = {\n",
    "                \"t-statistic\": np.nan,\n",
    "                \"p-value\": np.nan,\n",
    "                \"mean_with_feature\": np.nan,\n",
    "                \"mean_without_feature\": np.nan,\n",
    "                \"sample_size_with\": len(group_with_feature),\n",
    "                \"sample_size_without\": len(group_without_feature)\n",
    "            }\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        # Calculate t-test\n",
    "    try:\n",
    "        t_stat, p_value = ttest_ind(\n",
    "            group_with_feature,\n",
    "            group_without_feature,\n",
    "            equal_var=False,  # Using Welch's t-test\n",
    "            nan_policy='omit'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error with feature: {feature}\")\n",
    "        # Store results\n",
    "        results[feature] = {\n",
    "            \"t-statistic\": t_stat,\n",
    "            \"p-value\": p_value,\n",
    "            \"mean_with_feature\": group_with_feature.mean(),\n",
    "            \"mean_without_feature\": group_without_feature.mean(),\n",
    "            \"sample_size_with\": len(group_with_feature),\n",
    "            \"sample_size_without\": len(group_without_feature)\n",
    "        }\n",
    "        p_values.append(p_value)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    # Apply multiple testing correction\n",
    "    valid_p_values = ~np.isnan(p_values)\n",
    "    if sum(valid_p_values) > 0:\n",
    "        _, adjusted_p_values, _, _ = multipletests(\n",
    "            p_values[valid_p_values],\n",
    "            alpha=alpha,\n",
    "            method=correction_method\n",
    "        )\n",
    "        \n",
    "        # Add adjusted p-values back to results\n",
    "        results_df['adjusted_p_value'] = np.nan\n",
    "        results_df.loc[valid_p_values, 'adjusted_p_value'] = adjusted_p_values\n",
    "    \n",
    "    # Sort by adjusted p-value, then regular p-value\n",
    "    results_df = results_df.sort_values(\n",
    "        ['adjusted_p_value', 'p-value'],\n",
    "        na_position='last'\n",
    "    )\n",
    "    \n",
    "    return results_df\n",
    "    \n",
    "\n",
    "def full_analyses(df, show_whole_df_at_start=False, feature_extraction_analyses=True, tag=\"\"):\n",
    "    print(\"#\"*10)\n",
    "    print(\"Full analyses\")\n",
    "    print(\"#\"*10)\n",
    "    print(\"\\n Sorted by best model\\n\")\n",
    "    if show_whole_df_at_start:\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            display(df.sort_values(\"Balanced_ACC\", ascending=False))\n",
    "    else:\n",
    "        display(df.sort_values(\"Balanced_ACC\", ascending=False))\n",
    "\n",
    "    print(\"\\nValue counts for label column\\n\")\n",
    "    display(df[\"label_col\"].value_counts())\n",
    "\n",
    "    print(\"\\nMean and std for Balanced_ACC\\n\")\n",
    "    overall_mean = df[\"Balanced_ACC\"].mean()\n",
    "    overall_std = df[\"Balanced_ACC\"].std()\n",
    "    overall_permutated_mean = df[\"Permutation_Balanced_ACC\"].mean()\n",
    "    overall_permutated_std = df[\"Permutation_Balanced_ACC\"].std()\n",
    "\n",
    "    display(f\"Overall mean: {overall_mean}, std: {overall_std}\")\n",
    "    display(f\"Overall permuted mean: {overall_permutated_mean}, std: {overall_permutated_std}\")\n",
    "    display(f\"Mean difference: {overall_mean-overall_permutated_mean}, std difference: {overall_std-overall_permutated_std}\")\n",
    "    df[\"Balanced_ACC\"].hist()\n",
    "    plt.title(\"Balanced Accuracy Distribution\")\n",
    "    plt.show()\n",
    "    df[\"Permutation_Balanced_ACC\"].hist()\n",
    "    plt.title(\"Permutation Balanced Accuracy Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    summary = get_statistics_summary(df, [\"Balanced_ACC\", \"Permutation_Balanced_ACC\"], sort_by=\"Balanced_ACC_mean\", ascending=False)\n",
    "    summary.plot(kind=\"bar\", y=\"Balanced_ACC_mean\", yerr=\"Balanced_ACC_std\", title=\"Balanced Accuracy by Label\")\n",
    "    plt.title(\"Balanced Accuracy by Label\")\n",
    "    plt.show()\n",
    "    print(f\"Sorted by mean {tag}\")\n",
    "    display(summary)\n",
    "    print(\"Sorted by max\")\n",
    "    display(summary.sort_values(\"Balanced_ACC_max\", ascending=False))\n",
    "\n",
    "    \n",
    "    print(\"\\nMean and std for Balanced_ACC by mri_table\\n\")\n",
    "    print(\"NOT FILTERED FOR FEATURE EXTRACTION\")\n",
    "    summary = get_statistics_summary(df, [\"Balanced_ACC\", \"Permutation_Balanced_ACC\"], group_by_col=\"mri_table\", sort_by=\"Balanced_ACC_mean\", ascending=False)\n",
    "    summary.plot(kind=\"bar\", y=\"Balanced_ACC_mean\", yerr=\"Balanced_ACC_std\", title=\"Balanced Accuracy by MRI Table\")\n",
    "    plt.show()\n",
    "    print(f\"Sorted by mean {tag}\")\n",
    "    display(summary)\n",
    "    print(\"Sorted by max\")\n",
    "    display(summary.sort_values(\"Balanced_ACC_max\", ascending=False))\n",
    "\n",
    "    summary[\"features\"] = summary.index.str.split('_')\n",
    "    top_n = 10\n",
    "    top_features = summary.nlargest(top_n, 'Balanced_ACC_mean')[\"features\"].explode().value_counts()\n",
    "    print(\"Feature Importance by Count:\")\n",
    "    display(top_features)\n",
    "    if feature_extraction_analyses:\n",
    "        print(\"FILTERED FOR FEATURE EXTRACTION - NO FEATURE EXTRACTION APPLIED\")\n",
    "        df_filterd_for_no_feature_extraction = df[df[\"Feature_extraction_applied\"]==False]\n",
    "        mri_table_summary_filtered = get_statistics_summary(df_filterd_for_no_feature_extraction, [\"Balanced_ACC\", \"Permutation_Balanced_ACC\"], group_by_col=\"mri_table\", sort_by=\"Balanced_ACC_mean\", ascending=False)\n",
    "        mri_table_summary_filtered.plot(kind=\"bar\", y=\"Balanced_ACC_mean\", yerr=\"Balanced_ACC_std\", title=\"Balanced Accuracy by MRI Table - No FE\")\n",
    "        plt.show()\n",
    "        print(f\"Sorted by mean {tag}\")\n",
    "        display(mri_table_summary_filtered)\n",
    "        print(\"Sorted by max\")\n",
    "        display(mri_table_summary_filtered.sort_values(\"Balanced_ACC_max\", ascending=False))\n",
    "\n",
    "        mri_table_summary_filtered[\"features\"] = mri_table_summary_filtered.index.str.split('_')\n",
    "        top_n = 10\n",
    "        top_features = mri_table_summary_filtered.nlargest(top_n, 'Balanced_ACC_mean')[\"features\"].explode().value_counts()\n",
    "        print(\"Feature Importance by Count:\")\n",
    "        display(top_features)\n",
    "        #feature_statistical_testing(mri_table_summary_filtered)\n",
    "        print(\"Combination counts for features\")\n",
    "        combination_counts = count_partial_combinations(mri_table_summary_filtered.nlargest(top_n, 'Balanced_ACC_mean')[[\"Balanced_ACC_mean\", \"features\"]])\n",
    "        display(combination_counts)\n",
    "\n",
    "        print(\"FILTERED FOR FEATURE EXTRACTION - FEATURE EXTRACTION APPLIED\")\n",
    "        df_filterd_for_feature_extraction = df[df[\"Feature_extraction_applied\"]==True]\n",
    "        mri_table_summary_filtered = get_statistics_summary(df_filterd_for_feature_extraction, [\"Balanced_ACC\", \"Permutation_Balanced_ACC\"], group_by_col=\"mri_table\", sort_by=\"Balanced_ACC_mean\", ascending=False)\n",
    "        mri_table_summary_filtered.plot(kind=\"bar\", y=\"Balanced_ACC_mean\", yerr=\"Balanced_ACC_std\", title=\"Balanced Accuracy by MRI Table - FE\")\n",
    "        mri_table_summary_filtered[\"features\"] = mri_table_summary_filtered.index.str.split('_')\n",
    "        top_n = 10\n",
    "        top_features = mri_table_summary_filtered.nlargest(top_n, 'Balanced_ACC_mean')[\"features\"].explode().value_counts()\n",
    "        print(\"Feature Importance by Count:\")\n",
    "        display(top_features)\n",
    "        #feature_statistical_testing(mri_table_summary_filtered)\n",
    "    print(\"\\nMean and std for Balanced_ACC by model type\\n\")\n",
    "    summary = get_statistics_summary(df, [\"Balanced_ACC\", \"Permutation_Balanced_ACC\"], group_by_col=\"model_type\", sort_by=\"Balanced_ACC_mean\", ascending=False)\n",
    "    summary.plot(kind=\"bar\", y=\"Balanced_ACC_mean\", yerr=\"Balanced_ACC_std\", title=\"Balanced Accuracy by Model Type\")\n",
    "    plt.show()\n",
    "    print(f\"Sorted by mean {tag}\")\n",
    "    display(summary)\n",
    "    print(\"Sorted by max\")\n",
    "    display(summary.sort_values(\"Balanced_ACC_max\", ascending=False))\n",
    "\n",
    "    if feature_extraction_analyses:\n",
    "        print(\"\\nPretraing and Feature Extraction analyses\")\n",
    "        pretraining_featureextraction_analysis(df)\n",
    "        \n",
    "        print(\"\\nPretraining and Finetuning analyses for DL Models\\n\")\n",
    "        df_deeplearning = df[~df[\"model_type\"].isin([\"Logistic Regression\", \"Random Forest\"])]\n",
    "        pretraining_featureextraction_analysis(df_deeplearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/home/esralenz/Dokumente/20_HITKIP/03_UKB/00_Git_Code/UK-B-CLIP/08_analyse_results/2025_01_12_14_08_43.csv')\n",
    "#df_raw = pd.read_csv('training_log_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw.sort_values(\"Balanced_ACC\", ascending=False).head(20)\n",
    "#if the number of pos < 350 drop the row\n",
    "#df_raw_filterd = df_raw[df_raw[\"number_of_pos\"]>350]\n",
    "#df_raw_filterd.sort_values(\"Balanced_ACC\", ascending=False).head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all df in the df csvs_deconfounded chekc if teh end on .csv\n",
    "path = \"00_results/tabpfn_age/\"\n",
    "csvs_deconfounded = [path + f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "df = pd.concat([pd.read_csv(file) for file in csvs_deconfounded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"/home/esralenz/Dokumente/20_HITKIP/03_UKB/00_Git_Code/UK-B-CLIP/08_analyse_results/CNN_schaefer/CNN_Sch√§fer21_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mri_table\"] = (\n",
    "    df[\"mri_table\"]\n",
    "    .str.replace(\"FC_100\", \"FC.100\", regex=False)\n",
    "    .str.replace(\"FC_25\", \"FC.25\", regex=False)\n",
    "    .str.replace(\"grey_matter\", \"grey.matter\", regex=False)\n",
    "    .str.replace(\".csv\", \"\", regex=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mri_table\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the two dataframes\n",
    "#df = pd.concat([df, df_raw_filterd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"model_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_col\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics_with_others_df_1 = df.groupby(\n",
    "    ['search_term'], as_index=False\n",
    ").agg({\n",
    "    'Accuracy': 'mean',\n",
    "    'AUC': 'mean',\n",
    "    'Balanced_ACC': 'mean',\n",
    "    'Permutation_Balanced_ACC': 'mean',\n",
    "    'cross_validation_count': 'mean',\n",
    "    # Keeping the first instance of other columns\n",
    "    'label_col': 'first',\n",
    "    'mri_table': 'first',\n",
    "    'test_set_size': 'first',\n",
    "    'Pretraining_applied': 'first',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics_with_others_df_1.sort_values(\"Balanced_ACC\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics_with_others_df = df.groupby(\n",
    "    ['search_term', 'model_type'], as_index=False\n",
    ").agg({\n",
    "    'Accuracy': 'mean',\n",
    "    'AUC': 'mean',\n",
    "    'Balanced_ACC': 'mean',\n",
    "    'Permutation_Balanced_ACC': 'mean',\n",
    "    'cross_validation_count': 'mean',\n",
    "    # Keeping the first instance of other columns\n",
    "    'label_col': 'first',\n",
    "    'mri_table': 'first',\n",
    "    'test_set_size': 'first',\n",
    "    'Pretraining_applied': 'first',\n",
    "})\n",
    "#make the label col fisrt column\n",
    "target_col = 'label_col'\n",
    "columns = [target_col] + [col for col in mean_metrics_with_others_df if col != target_col]\n",
    "mean_metrics_with_others_df = mean_metrics_with_others_df[columns]\n",
    "df = mean_metrics_with_others_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"label_col\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all labels that have sex_column in there\n",
    "df[df[\"label_col\"].str.contains(\"sex_balanced\")][\"label_col\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"mri_table\")[\"Balanced_ACC\"].agg([\"mean\", \"max\"]).sort_values(\"mean\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the full analyses for the different tags\n",
    "full_analyses(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in df[\"tag\"].unique():\n",
    "    print(\"-\"*30)\n",
    "    print(f\"\\nAnalyses for tag: {tag}\")\n",
    "    df_tag = df[df[\"tag\"] == tag]\n",
    "    full_analyses(df_tag, show_whole_df_at_start=False, tag=tag)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for sex_balanced rows\n",
    "\n",
    "#df_sexbalanced = df[df[\"label_col\"].str.contains(\"sex_balanced\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the row where in mri_table there is a \"Fc\" \n",
    "#df = df[~df[\"mri_table\"].str.contains(\"FC\")]\n",
    "#full_analyses(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in df[\"tag\"].unique():\n",
    "    print(\"-\"*30)\n",
    "    print(f\"\\nAnalyses for tag: {tag}\")\n",
    "    df_tag = df[df[\"tag\"] == tag]\n",
    "    full_analyses(df_tag)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2_gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
